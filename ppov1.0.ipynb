{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np\n",
    "#import tensorflow_probability as tfp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <TimeLimit<CartPoleEnv<CartPole-v0>>>\n",
      "State shape: (4,)\n",
      "Action shape: ()\n",
      "action space Discrete(2) observation space : Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "env_name = 'CartPole-v0' #'LunarLanderContinuous-v2' #'Pendulum-v0' #   #'MountainCarContinuous-v0' #\n",
    "\n",
    "env = gym.make(env_name)\n",
    "#env.render()\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "#env = env.reset()\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "print(state_dim)\n",
    "print(n_actions)\n",
    "tf.random.set_seed(336699)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.v = tf.keras.layers.Dense(1,activation='linear')\n",
    "        \n",
    "    def call(self,state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        val = self.v(x)\n",
    "        return tf.squeeze(val,axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self,state_dim, n_actions):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(256,activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(128,activation='relu')\n",
    "        self.prob = tf.keras.layers.Dense(n_actions, activation = 'softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        probs = self.prob(x)\n",
    "        return tf.squeeze(probs)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case of ppo the buffer has to be cleared after every episode, lest the previous samples affect the working of the algorithm\n",
    "\n",
    "class ReplayBufferPPO:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, prob, reward, next_state, done, val):\n",
    "        item = (state, action, prob, reward, next_state, done, val)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = len(self.buffer) #np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in range(idxs)]\n",
    "        states, actions, probs, rewards, next_states, done_flags, vals = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32),np.array(probs, np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags), np.array(vals, np.float32)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, probs, rewards, next_states, done_flags, vals = self.sample(32)\n",
    "        #print(type(states))\n",
    "        #print(f' states {states} actions : {actions} probs : {probs} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags} vals : {vals}')\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1)) # ,act_dim))\n",
    "\n",
    "        probs = np.reshape(probs, (-1,act_dim))\n",
    "\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        vals = np.reshape(vals, (-1))\n",
    "\n",
    "        #print(f' states {states} actions : {actions} probs : {probs} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags} vals : {vals}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.int32) #used for indexing\n",
    "        prob_ts = tf.convert_to_tensor(probs,dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "        val_ts = tf.convert_to_tensor(vals, dtype=tf.float32)\n",
    "    \n",
    "        #print(f'Tensor states {state_ts} actions : {action_ts} probs: {prob_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags} vals : {val_ts}')\n",
    "\n",
    "        return state_ts, action_ts, prob_ts, reward_ts, next_state_ts, done_flags, val_ts\n",
    "    def generate_trajectory(self,env, policy, critic, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        self.buffer = [] #has to be cleared or the working of the algorithms will be affected\n",
    "        done = False\n",
    "        while not done:\n",
    "        #for _ in range(n_steps):\n",
    "            prob = policy(state[np.newaxis])\n",
    "            val = critic(state[np.newaxis])\n",
    "            prob = np.array(prob,np.float32)\n",
    "            action = np.random.choice(n_actions, p=prob)\n",
    "            #action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} prob: {prob} reward {reward} next state : {next_state} done : {done} val : {val}')\n",
    "            buffer.add(state, action, prob, reward, next_state, done, val)\n",
    "            state = next_state\n",
    "buffer = ReplayBufferPPO(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentPPO:\n",
    "    def __init__(self, state_dim, n_actions, clip_val=0.2, learning_rate = 1e-3, gamma = 0.99):\n",
    "        self.learning_rate_critic = learning_rate\n",
    "        self.learning_rate_policy = learning_rate      \n",
    "        self.gamma = gamma        \n",
    "        self.act_dim = n_actions\n",
    "        self.state_dim = state_dim\n",
    "        self.clip_val = clip_val\n",
    "\n",
    "        self.policy = Policy(state_dim,n_actions)\n",
    "        self.critic = Critic(state_dim)\n",
    "\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(self.learning_rate_critic)\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(self.learning_rate_policy)\n",
    "        \n",
    "\n",
    "    def process(self,buffer):\n",
    "        states, actions, probs, rewards, next_states, dones, vals = buffer.to_tensors(self.state_dim, self.act_dim)       \n",
    "        rtg = np.zeros(len(rewards))\n",
    "        rtg[len(rewards)-1] = rewards[len(rewards)-1]\n",
    "        for i in range(len(rewards)-2,-1,-1):           \n",
    "            rtg[i] = rewards[i] + self.gamma*rtg[i+1]\n",
    "\n",
    "        #print('********')\n",
    "        #rtg = rtg.numpy()\n",
    "        vals = vals.numpy()\n",
    "        advantages = rtg - vals        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "        \n",
    "        rtg = (rtg - rtg.mean())/(rtg.std()+1e-6)        \n",
    "        \n",
    "        rtg_ts = tf.convert_to_tensor(rtg, dtype= tf.float32)\n",
    "        advantages_ts = tf.convert_to_tensor(advantages, dtype=tf.float32)\n",
    "        #print(f' rtg tensor : {rtg_ts} advantages tensor : {advantages_ts}')\n",
    "        \n",
    "        return rtg_ts, advantages_ts, states, actions, probs \n",
    "    \n",
    "    def critic_loss(self, states, rtg):\n",
    "        with tf.GradientTape() as tape:\n",
    "            vals = self.critic(states, training = True)\n",
    "            \n",
    "            c_loss = tf.reduce_mean((vals - rtg)**2) # tf.keras.losses.MSE(vals,rtg) # tf.reduce_mean((vals - rtg)**2)\n",
    "            #print(f'vals : {vals} rtg : {rtg} closs : {c_loss}')\n",
    "        grads = tape.gradient(c_loss,self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads,self.critic.trainable_variables))\n",
    "\n",
    "        return c_loss\n",
    "\n",
    "    def policy_loss(self, states, advantages, actions, old_probs, c_loss):\n",
    "        surrogate1 = []\n",
    "        surrogate2 = []\n",
    "        \n",
    "        #print(f'states: {states} old probs : {old_probs} actions : {actions}')\n",
    "        with tf.GradientTape() as tape:\n",
    "            new_probs = self.policy(states, training=True)\n",
    "            #entropy = tf.reduce_mean(- tf.math.multiply(new_probs, tf.math.log(new_probs)))\n",
    "            #print(f'new probs : {new_probs}')\n",
    "            for pb, op, adv, act in zip(new_probs, old_probs, advantages, actions):               \n",
    "                ratio = tf.divide(pb[act], op[act])                \n",
    "                s1 = tf.multiply(ratio, adv)\n",
    "                s2 = tf.multiply(tf.clip_by_value(ratio, 1.0 - self.clip_val, 1.0 + self.clip_val),adv)\n",
    "\n",
    "                #print(f' pb : {pb} op : {op} adv :{adv} act :{act} ratio : {ratio} s1 :{s1} s2:{s2}')\n",
    "\n",
    "                surrogate1.append(s1)\n",
    "                surrogate2.append(s2)\n",
    "            surrogate1 = tf.stack(surrogate1)\n",
    "            surrogate2 = tf.stack(surrogate2)\n",
    "\n",
    "            #print(f'surrogate 1 : {surrogate1} surrogate 2 :{surrogate2}')\n",
    "\n",
    "            p_loss = tf.negative(tf.reduce_mean((tf.minimum(surrogate1,surrogate2)))) # - (0.3 * c_loss)) #,0.001*entropy)))\n",
    "            #print(f' P loss : {p_loss}')\n",
    "        grads = tape.gradient(p_loss, self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        return p_loss\n",
    "\n",
    "    def train_episode(self,env, buffer, repeat_train_steps=10):\n",
    "        loss_c, loss_p = [], []\n",
    "        buffer.generate_trajectory(env, self.policy, self.critic)\n",
    "        rtg, advantages, states, actions, probs = self.process(buffer)        \n",
    "        for steps in range(repeat_train_steps):\n",
    "            c_loss = self.critic_loss(states,rtg)\n",
    "            p_loss = self.policy_loss(states, advantages, actions, probs, c_loss)\n",
    "            loss_c.append(c_loss)\n",
    "            loss_p.append(p_loss)\n",
    "        return loss_c, loss_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reward(env,agent):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(agent.policy(np.array([state])).numpy())\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer policy is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "avg reward after iteration 0 = 35.4 critic loss =  -0.017462752759456635 policy loss = 0.6538090705871582 \n",
      "avg reward after iteration 1 = 41.4 critic loss =  -0.03546004742383957 policy loss = 1.3751798868179321 \n",
      "avg reward after iteration 2 = 9.6 critic loss =  -0.04154008626937866 policy loss = 0.8756036758422852 \n",
      "avg reward after iteration 3 = 9.6 critic loss =  0.0054579321295022964 policy loss = 0.5638291239738464 \n",
      "avg reward after iteration 4 = 34.6 critic loss =  -0.02691325917840004 policy loss = 0.4888705313205719 \n",
      "avg reward after iteration 5 = 57.2 critic loss =  -0.03614586964249611 policy loss = 0.5200445652008057 \n",
      "avg reward after iteration 6 = 61.2 critic loss =  -0.010958828032016754 policy loss = 0.5935214757919312 \n",
      "avg reward after iteration 7 = 46.0 critic loss =  -0.022521691396832466 policy loss = 0.732373058795929 \n",
      "avg reward after iteration 8 = 52.4 critic loss =  -0.011327952146530151 policy loss = 0.40940824151039124 \n",
      "avg reward after iteration 9 = 44.6 critic loss =  0.0022335131652653217 policy loss = 0.20401103794574738 \n",
      "avg reward after iteration 10 = 68.4 critic loss =  -0.0031350310891866684 policy loss = 0.11250416934490204 \n",
      "avg reward after iteration 11 = 117.8 critic loss =  -0.012868976220488548 policy loss = 2.474416971206665 \n",
      "avg reward after iteration 12 = 136.0 critic loss =  -0.009376171045005322 policy loss = 0.6848312616348267 \n",
      "avg reward after iteration 13 = 183.4 critic loss =  -0.00807378999888897 policy loss = 0.40675657987594604 \n",
      "avg reward after iteration 14 = 115.8 critic loss =  -0.020816951990127563 policy loss = 0.6108356714248657 \n",
      "avg reward after iteration 15 = 123.0 critic loss =  -0.011118397116661072 policy loss = 0.19699086248874664 \n",
      "avg reward after iteration 16 = 136.0 critic loss =  -0.006862389389425516 policy loss = 7.504383087158203 \n",
      "avg reward after iteration 17 = 200.0 critic loss =  -0.018643442541360855 policy loss = 1.1437976360321045 \n",
      " total rewards = [35.4, 41.4, 9.6, 9.6, 34.6, 57.2, 61.2, 46.0, 52.4, 44.6, 68.4, 117.8, 136.0, 183.4, 115.8, 123.0, 136.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "with tf.device('GPU:0'):\n",
    "    avg_rewards = []\n",
    "    agent = AgentPPO(state_dim,n_actions)\n",
    "    for epoch in range(200):\n",
    "        policy_loss, critic_loss =  agent.train_episode(env,buffer,10)\n",
    "        avg_reward = np.mean([test_reward(env, agent) for _ in range(5)])\n",
    "\n",
    "        print(f'avg reward after iteration {epoch} = {avg_reward} critic loss =  {np.mean(np.array(critic_loss[-10:]))} policy loss = {np.mean(np.array(policy_loss[-10:]))} ')\n",
    "        avg_rewards.append(avg_reward)\n",
    "        if avg_reward > 190:\n",
    "            break\n",
    "    print(f' total rewards = {avg_rewards}')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
