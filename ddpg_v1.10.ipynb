{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Deep deterministic policy gradient\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "from IPython.display import HTML\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <PendulumEnv<Pendulum-v0>>\n",
      "State shape: (3,)\n",
      "Action shape: (1,)\n",
      "action space Box([-2.], [2.], (1,), float32) observation space : Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "action space bound :[-2.], [2.]\n",
      "action limit = 2.0 dimension 1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "env_name = 'Pendulum-v0' # 'AntBulletEnv-v0' #'LunarLanderContinuous-v2' #'Pendulum-v0' #   #'MountainCarContinuous-v0' #\n",
    "\n",
    "env = make_env(env_name)\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "#env = env.reset()\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        #self.initializer = tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "        self.fc1 = tf.keras.layers.Dense(500, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(500, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1, activation='linear')#, kernel_initializer=self.initializer)\n",
    "        \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis = -1)\n",
    "        #print(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim, act_limit):\n",
    "        super(Policy, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb2 = tf.keras.layers.BatchNormalization()\n",
    "        self.actor = tf.keras.layers.Dense(act_dim)\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fcb2(x)\n",
    "        x = self.actor(x)\n",
    "        x = tf.keras.activations.tanh(x)  # to output in range(-1,1)\n",
    "        x = self.act_limit * x\n",
    "        return x\n",
    "    \n",
    "    def get_action1(self, s):\n",
    "        action = self.call(s)\n",
    "        #print(f'action :{action}')\n",
    "        normal_dist = tfp.distributions.Normal(0,0.1)\n",
    "        sml = normal_dist.sample()\n",
    "        sml = tf.clip_by_value(sml, -0.02,0.02)\n",
    "        #print(f'sample : {sml}')\n",
    "        action += sml\n",
    "        action = tf.clip_by_value(action, -self.act_limit,self.act_limit)\n",
    "        return action\n",
    "    \n",
    "    def get_action(self,s):\n",
    "        #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.call(s)\n",
    "        ##print(actions)\n",
    "        \n",
    "        actions += tf.clip_by_value(tf.random.normal(shape=[act_dim], mean=0.0, stddev=0.1),-0.2,0.2)\n",
    "\n",
    "        actions = (tf.clip_by_value(actions, -self.act_limit , self.act_limit))\n",
    "        ##print(f'actions in ac : {actions}')\n",
    "        return actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        ##print(f'Tensor states {state_ts} actions : {action_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "\n",
    "        return state_ts, action_ts, reward_ts, next_state_ts, done_flags\n",
    "    def initialize_replay_buffer(self,env, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentDDPG:\n",
    "    def __init__(self,env, act_dim,act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.learning_rate_critic = learning_rate\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic = Critic(state_dim,act_dim)\n",
    "        self.target_critic = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.policy = Policy(state_dim,act_dim,act_limit)\n",
    "        self.target_policy = Policy(state_dim,act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "        _ = self.critic(s,a[np.newaxis])\n",
    "        _ = self.target_critic(s,a[np.newaxis])\n",
    "        _ = self.policy(s)\n",
    "        _ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic.set_weights(self.critic.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "\n",
    "        self.target_critic.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "\n",
    "    #@tf.function\n",
    "    def compute_q_loss(self, states, actions, rewards, next_states, dones, gamma=0.99):\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_qvals = self.critic(states,actions)\n",
    "            \n",
    "            next_acts = self.target_policy(next_states)\n",
    "            next_acts += tf.clip_by_value(tf.random.normal(shape = next_acts.shape, mean=0, stddev=0.5),-0.5,0.5)\n",
    "\n",
    "            next_qval = self.critic(next_states, next_acts)\n",
    "            target = rewards + gamma*(1-dones)*next_qval\n",
    "\n",
    "            loss = tf.reduce_mean((pred_qvals - target)**2)\n",
    "\n",
    "            #print(f' predicted q vals : {pred_qvals} __ next qvals : {next_qval}  target : {target}  loss : {loss}')\n",
    "        grads = tape.gradient(loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(grads, self.critic.trainable_variables))\n",
    "        return loss\n",
    "    \n",
    "    #@tf.function\n",
    "    def compute_p_loss(self, states):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_val = self.critic(states,self.policy(states))\n",
    "            loss = - tf.reduce_mean(pred_val)            \n",
    "            #print(f'pred val : {pred_val} loss : {loss}')\n",
    "        grads = tape.gradient(loss, self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads, self.policy.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def train_step(self):\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "\n",
    "        c_loss = self.compute_q_loss(states,actions, rewards, next_states, dones)\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        p_loss = self.compute_p_loss(states)\n",
    "        \n",
    "        self.critic.trainable = True\n",
    "        \n",
    "        self.polyak_update(self.target_critic, self.critic)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss\n",
    "    \n",
    "    def test_agent(self, env, num_test_episodes=5, max_ep_len=500):\n",
    "        ep_rets, ep_lens = [], []\n",
    "        #print(num_test_episodes, env)\n",
    "        for j in range(num_test_episodes):\n",
    "            state, done, ep_ret, ep_len = env.reset(), False, 0, 0\n",
    "            #print(f' state : {state}')\n",
    "            while not(done or (ep_len == max_ep_len)):\n",
    "                # Take deterministic actions at test time (noise_scale=0)\n",
    "                #print(f'state :: {state}')\n",
    "                action = self.policy(state[np.newaxis])\n",
    "                #action = tf.squeeze(action)\n",
    "                #print(action)\n",
    "                state, reward, done, _ = env.step(action[0]) #env.action_space.sample()) #action[np.newaxis]) #)agent.get_action(state, 0))\n",
    "                ep_ret += reward\n",
    "                ep_len += 1\n",
    "            ep_rets.append(ep_ret)\n",
    "            ep_lens.append(ep_len)\n",
    "        return np.mean(ep_rets), np.mean(ep_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "returns =  -3773.25827057924 episode 0 returns : -3641.319586667433 episode len : 400.0 avg c_loss 7.531282901763916 avg p_loss 0.9993664622306824 \n",
      "return in episode : 0 : -817.6192558694623\n",
      "returns =  -3836.544245212933 episode 10 returns : -2831.87489573738 episode len : 400.0 avg c_loss 7.843446254730225 avg p_loss 38.08415603637695 \n",
      "return in episode : 10 : -617.2887219450791\n",
      "returns =  -3884.422033633696 episode 20 returns : -2960.5682078967257 episode len : 400.0 avg c_loss 2.3843154907226562 avg p_loss 530.8468627929688 \n",
      "return in episode : 20 : -692.7203293890454\n",
      "returns =  -2988.490054765366 episode 30 returns : -1580.2233086665624 episode len : 400.0 avg c_loss 0.8702407479286194 avg p_loss 654.0594482421875 \n",
      "return in episode : 30 : -571.2156137910742\n",
      "returns =  -2063.6663882199105 episode 40 returns : -1296.5932815710164 episode len : 400.0 avg c_loss 1.228826880455017 avg p_loss 396.3433532714844 \n",
      "return in episode : 40 : -249.4104487908584\n",
      "returns =  -1393.1266032952951 episode 50 returns : -689.993607347453 episode len : 400.0 avg c_loss 1.0782512426376343 avg p_loss 314.57421875 \n",
      "return in episode : 50 : -248.09037833523763\n",
      "returns =  -1043.45481242494 episode 60 returns : -785.3820244358146 episode len : 400.0 avg c_loss 0.7214311957359314 avg p_loss 240.24481201171875 \n",
      "return in episode : 60 : -347.25099269050315\n",
      "returns =  -2450.5554152879017 episode 70 returns : -1875.3136018402724 episode len : 400.0 avg c_loss 3.1794750690460205 avg p_loss 241.35494995117188 \n",
      "return in episode : 70 : -274.34788871788976\n",
      "returns =  -2776.9017709882746 episode 80 returns : -1616.36886954084 episode len : 400.0 avg c_loss 2.8115203380584717 avg p_loss 274.5490417480469 \n",
      "return in episode : 80 : -398.0073225073126\n",
      "returns =  -969.4894600861577 episode 90 returns : -928.9699577759891 episode len : 400.0 avg c_loss 0.7822163105010986 avg p_loss 265.5381164550781 \n",
      "return in episode : 90 : -242.95843866606646\n",
      "returns =  -982.0222715606162 episode 100 returns : -773.4173985502837 episode len : 400.0 avg c_loss 0.8564589619636536 avg p_loss 193.6973419189453 \n",
      "return in episode : 100 : -252.81712633891988\n",
      "returns =  -1317.5127369113625 episode 110 returns : -152.84663747636483 episode len : 400.0 avg c_loss 0.9834864735603333 avg p_loss 122.77489471435547 \n",
      "return in episode : 110 : -122.14727111909419\n",
      "returns =  -2855.068471876883 episode 120 returns : -133.9349788237002 episode len : 400.0 avg c_loss 3.7594032287597656 avg p_loss 220.07762145996094 \n",
      "return in episode : 120 : -496.0861908728138\n",
      "returns =  -771.2616630888124 episode 130 returns : -254.16348937071734 episode len : 400.0 avg c_loss 1.0549763441085815 avg p_loss 164.2735595703125 \n",
      "return in episode : 130 : -3.5642352511616666\n",
      "returns =  -720.9529035678534 episode 140 returns : -1701.222017041927 episode len : 400.0 avg c_loss 0.9164817333221436 avg p_loss 118.03056335449219 \n",
      "return in episode : 140 : -518.2022268269752\n",
      "returns =  -2393.591632799782 episode 150 returns : -1635.0189841117392 episode len : 400.0 avg c_loss 4.452218532562256 avg p_loss 244.72784423828125 \n",
      "return in episode : 150 : -748.9808151789593\n",
      "returns =  -1215.7779120541948 episode 160 returns : -379.6684129884567 episode len : 400.0 avg c_loss 1.28121018409729 avg p_loss 187.8657989501953 \n",
      "return in episode : 160 : -499.97588342789805\n",
      "returns =  -361.6410981741172 episode 170 returns : -183.97809167506225 episode len : 400.0 avg c_loss 0.712024986743927 avg p_loss 80.80009460449219 \n",
      "return in episode : 170 : -0.8547086226925721\n",
      "returns =  -1711.7117341853034 episode 180 returns : -253.30956559556552 episode len : 400.0 avg c_loss 1.7868658304214478 avg p_loss 85.28314208984375 \n",
      "return in episode : 180 : -254.14505874401812\n",
      "returns =  -330.00580602463145 episode 190 returns : -625.9910803317587 episode len : 400.0 avg c_loss 0.5509289503097534 avg p_loss 60.84626388549805 \n",
      "return in episode : 190 : -390.9333387579457\n",
      "returns =  -1262.2717455848203 episode 200 returns : -124.0302263382928 episode len : 400.0 avg c_loss 1.1037460565567017 avg p_loss 87.86360931396484 \n",
      "return in episode : 200 : -2.0160219282515937\n",
      "returns =  -813.8356768423831 episode 210 returns : -754.5108645340543 episode len : 400.0 avg c_loss 0.6891032457351685 avg p_loss 52.91969299316406 \n",
      "return in episode : 210 : -234.77357547101704\n",
      "returns =  -2161.700105537266 episode 220 returns : -528.6622565250071 episode len : 400.0 avg c_loss 3.4913923740386963 avg p_loss 189.2633056640625 \n",
      "return in episode : 220 : -266.4347993647535\n",
      "returns =  -219.05915414600722 episode 230 returns : -248.43588695622967 episode len : 400.0 avg c_loss 0.5344399213790894 avg p_loss 74.31455993652344 \n",
      "return in episode : 230 : -128.02261470271563\n",
      "returns =  -1572.085790073626 episode 240 returns : -1893.6635594331924 episode len : 400.0 avg c_loss 1.984134554862976 avg p_loss 97.35257720947266 \n",
      "return in episode : 240 : -372.9028925600587\n"
     ]
    },
   ],
   "source": [
    "with tf.device('GPU:0'):   \n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentDDPG(env, act_dim,act_limit, state_dim)\n",
    "    test_env = make_env(env_name)\n",
    "    state = env.reset()\n",
    "    ret = 0 \n",
    "\n",
    "    c_loss, c_loss2, p_loss,v_loss = [],[],[],[]\n",
    "    rets = []\n",
    "    for episode in range(1000):\n",
    "    #state = state[np.newaxis]\n",
    "    #for step in range(200000):\n",
    "        for step in range(500):   \n",
    "            action = agent.policy.get_action(state[np.newaxis]) \n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "\n",
    "            ret +=  reward\n",
    "            if (done or step % 400 == 0) and step != 0:\n",
    "                state = env.reset()\n",
    "                #print( f' return : {ret} after : {step}')\n",
    "                rets.append(ret)\n",
    "                ret = 0\n",
    "                done = True\n",
    "            else:           \n",
    "                buffer.add(state, action[0], reward, next_state, done)\n",
    "\n",
    "                state = next_state\n",
    "                if step % 100 == 0:\n",
    "                    for i in range(50):\n",
    "                        loss_p, loss_c = agent.train_step()\n",
    "                        c_loss.append(loss_c)\n",
    "                        p_loss.append(loss_p)                    \n",
    "        if episode % 10 == 0:\n",
    "            ep_ret, ep_len = agent.test_agent(test_env, 5, 400)\n",
    "            print(f'returns =  {np.mean(np.array(rets))} episode {episode} returns : {ep_ret} episode len : {ep_len} avg c_loss {np.mean(np.array(c_loss, np.float32))} avg p_loss {np.mean(np.array(p_loss, np.float32))} ')\n",
    "            c_loss, p_loss = [], []\n",
    "            rets = []\n",
    "            print(f'return in episode : {episode} : {ret}')"
   ]
  }
 
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fae32586ff03f47a58c920ef61c568276f9b28096f0a46988759f2e818341ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
