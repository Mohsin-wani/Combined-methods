{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##soft actor critic using value and Q funtion\n",
    "\n",
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import tensorflow_probability as tfp\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "import pybullet_envs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <LunarLanderContinuous<LunarLanderContinuous-v2>>\n",
      "State shape: (8,)\n",
      "Action shape: (2,)\n",
      "action space Box([-1. -1.], [1. 1.], (2,), float32) observation space : Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "action space bound :[-1. -1.], [1. 1.]\n",
      "action limit = 1.0 dimension 2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env_name = 'LunarLanderContinuous-v2' #'Pendulum-v0' #   #'MountainCarContinuous-v0' #'AntBulletEnv-v0' #\n",
    "\n",
    "env = make_env(env_name)\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "#env = env.reset()\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.initializer = tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(1, activation='linear', kernel_initializer=self.initializer)\n",
    "        \n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis = -1)\n",
    "        #print(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.out(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim, act_limit, clip_val_min = -20, clip_val_max = 2):\n",
    "        super(Policy, self).__init__()\n",
    "        self.clip_val_min = clip_val_min\n",
    "        self.clip_val_max = clip_val_max\n",
    "\n",
    "        self.initializer = tf.keras.initializers.RandomUniform(minval=-0.003, maxval=0.003)\n",
    "        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.mean = tf.keras.layers.Dense(act_dim, activation = 'linear', kernel_initializer=self.initializer)\n",
    "        self.log_std_dev = tf.keras.layers.Dense(act_dim, activation='linear', kernel_initializer=self.initializer)\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        mu = self.mean(x)\n",
    "        log_sigma = self.log_std_dev(x)\n",
    "        \n",
    "        log_sigma = tf.clip_by_value(log_sigma,clip_value_min=self.clip_val_min, clip_value_max=self.clip_val_max)\n",
    "        return mu, log_sigma\n",
    "\n",
    "    def eval(self, state, eps = 1e-6):\n",
    "        mu, log_sigma = self.call(state)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "\n",
    "        dist = tfp.distributions.Normal(mu, sigma)\n",
    "        action_ = dist.sample()\n",
    "\n",
    "        # Apply the tanh squashing to keep the gaussian bounded in (-1,1)\n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        # Calculate the log probability\n",
    "        log_pi_ = dist.log_prob(action_)\n",
    "        # Change log probability to account for tanh squashing as mentioned in paper\n",
    "       \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(1 - action**2 + eps), axis=1, keepdims=True)\n",
    "\n",
    "        return action, log_pi\n",
    "    \n",
    "    def eval_n(self, state, eps = 1e-6):\n",
    "        mu, log_sigma = self.call(state)\n",
    "        std_dev = tf.exp(log_sigma)\n",
    "\n",
    "        normal = tfp.distributions.Normal(0,1)\n",
    "        z = normal.sample() \n",
    "\n",
    "        action = tf.tanh( mu + std_dev * z)\n",
    "        dist = tfp.distributions.Normal(mu, std_dev)\n",
    "        log_prob = dist.log_prob(mu + std_dev*z) - tf.math.log(1-action**2 + eps)\n",
    "\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        ##print(f'Tensor states {state_ts} actions : {action_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "\n",
    "        return state_ts, action_ts, reward_ts, next_state_ts, done_flags\n",
    "    def initialize_replay_buffer(self,env, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentSAC:\n",
    "    def __init__(self, act_dim,act_limit, state_dim, learning_rate = 1e-3, alpha = 0.2, gamma = 0.99, polyak = 0.95):\n",
    "        #super(self).__init__()\n",
    "        self.learning_rate_critic = learning_rate\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "\n",
    "        self.critic1 = Critic(state_dim, act_dim)\n",
    "        self.critic2 = Critic(state_dim, act_dim)\n",
    "\n",
    "        self.policy = Policy(state_dim,act_dim,act_limit)\n",
    "\n",
    "        self.target_critic1 = Critic(state_dim, act_dim)\n",
    "        self.target_critic2 = Critic(state_dim, act_dim)\n",
    "\n",
    "        #### instantiate networks\n",
    "        state = env.reset() \n",
    "        act = env.action_space.sample()\n",
    "\n",
    "        _ = self.critic1(state[np.newaxis],act[np.newaxis])\n",
    "        _ = self.critic2(state[np.newaxis],act[np.newaxis])\n",
    "        _ = self.target_critic1(state[np.newaxis],act[np.newaxis])\n",
    "        _ = self.target_critic2(state[np.newaxis],act[np.newaxis])\n",
    "\n",
    "        _, _ = self.policy(state[np.newaxis])\n",
    "    \n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(self.learning_rate_critic)\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(self.learning_rate_policy)\n",
    "\n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, weights_target in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak*weights_target+(1-self.polyak)*weights\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "\n",
    "    def compute_q_loss(self, states, actions, rewards, next_states, dones):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            q_val1 = self.critic1(states,actions)\n",
    "            q_val2 = self.critic2(states,actions)\n",
    "\n",
    "            acts, log_pis = self.policy.eval_n(next_states) #should be next_states\n",
    "\n",
    "            #print(f'log pis before reduction : {log_pis}')\n",
    "\n",
    "            q_tar1 = self.target_critic1(next_states,acts)            \n",
    "            q_tar2 = self.target_critic2(next_states,acts)\n",
    "\n",
    "            min_tar = tf.minimum(q_tar1,q_tar2)\n",
    "\n",
    "            log_pis = tf.squeeze(log_pis)\n",
    "            #print(f'log pis after squeeze: {log_pis}')\n",
    "            log_pis = tf.reduce_mean(log_pis,axis=-1)\n",
    "            #print(f'log pis after reduction : {log_pis}')\n",
    "\n",
    "            softq = min_tar - self.alpha * log_pis\n",
    "            target = rewards + self.gamma*(1-dones)*softq\n",
    "\n",
    "            critic_loss1 = tf.reduce_mean((q_val1 - target)**2)\n",
    "            critic_loss2 = tf.reduce_mean((q_val2 - target)**2)\n",
    "\n",
    "            #print(f'q_val1 : {q_val1} qval2 : {q_val2} q_tar1 : {q_tar1} qtar2 : {q_tar2} acts = {acts} logpis = {log_pis} min_tar = : {min_tar} softq : {softq} target: {target} critic loss :{critic_loss1} :: {critic_loss2}' )\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1, self.critic1.trainable_variables))\n",
    "\n",
    "        grads2 = tape2.gradient(critic_loss2,self.critic2.trainable_variables)\n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    def compute_p_loss(self, states):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions, log_pis = self.policy.eval_n(states)\n",
    "            q_val1 = self.critic1(states,actions)\n",
    "            q_val2 = self.critic2(states, actions)\n",
    "\n",
    "            min_q = tf.minimum(q_val1,q_val2)\n",
    "\n",
    "            #print(f'log pis before reduction : {log_pis}')\n",
    "            log_pis = tf.squeeze(log_pis)\n",
    "            log_pis = tf.reduce_mean(log_pis,axis=-1)\n",
    "\n",
    "            softq = min_q - self.alpha * log_pis\n",
    "            actor_loss = - tf.reduce_mean(softq)\n",
    "            #print(f'q_val1 : {q_val1} qval2 : {q_val2} log_pis : {log_pis} softq : {softq} actor loss : {actor_loss}')\n",
    "        grads = tape.gradient(actor_loss, self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        return actor_loss\n",
    "    \n",
    "    def train(self):\n",
    "        states, actions, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions,rewards,next_states, dones)\n",
    "        self.critic1.trainable = False\n",
    "        self.critic2.trainable = False\n",
    "        p_loss = self.compute_p_loss(states)\n",
    "\n",
    "        self.critic1.trainable = True\n",
    "        self.critic2.trainable = True\n",
    "        \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 avg reward : -20.86916624335529\n",
      "after 20 avg reward : -28.52709590437225\n",
      "after 30 avg reward : -41.28292956990454\n",
      "after 40 avg reward : 78.6040414986492\n",
      "after 50 avg reward : 54.827848405855875\n",
      "after 60 avg reward : -76.78229645959075\n",
      "after 70 avg reward : -35.146489019754185\n",
      "after 80 avg reward : 62.86156640464973\n",
      "after 90 avg reward : -245.68267751463532\n",
      "after 100 avg reward : -45.47875407530154\n",
      "after 110 avg reward : 11.129116879513196\n",
      "after 120 avg reward : -50.92521181631621\n",
      "after 130 avg reward : 17.114246862745507\n",
      "after 140 avg reward : 34.64452619020713\n",
      "after 150 avg reward : -25.388663695123075\n",
      "after 160 avg reward : 65.98124797652007\n",
      "after 170 avg reward : 65.52826287499302\n",
      "after 180 avg reward : 23.328789487261584\n",
      "after 190 avg reward : 54.101622321708376\n",
      "after 200 avg reward : 42.84557603224668\n",
      "after 210 avg reward : 15.546715747631643\n",
      "after 220 avg reward : 6.275468224453639\n",
      "after 230 avg reward : 13.937769320497974\n",
      "after 240 avg reward : 46.59302864438244\n",
      "after 250 avg reward : 77.48880632408161\n",
      "after 260 avg reward : 109.7859158177326\n",
      "after 270 avg reward : 60.865591428231404\n",
      "after 280 avg reward : 96.39571091112201\n",
      "after 290 avg reward : 97.24677596808804\n",
      "after 300 avg reward : 74.81211158673571\n",
      "after 310 avg reward : 40.52158613643893\n",
      "after 320 avg reward : 38.27606928207683\n",
      "after 330 avg reward : -35.770669094604884\n",
      "after 340 avg reward : 55.85605680792563\n",
      "after 350 avg reward : 101.33538536228279\n",
      "after 360 avg reward : 89.57643554069695\n",
      "after 370 avg reward : 50.39125522339189\n",
      "after 380 avg reward : 53.31848246055601\n",
      "after 390 avg reward : 70.60145768119601\n",
      "after 400 avg reward : -72.80331770960245\n",
      "after 410 avg reward : 25.0582793939819\n",
      "after 420 avg reward : 32.83842927370988\n",
      "after 430 avg reward : 28.427724310228513\n",
      "after 440 avg reward : 7.086721056747045\n",
      "after 450 avg reward : 36.98910933863518\n",
      "after 460 avg reward : 54.595074622377886\n",
      "after 470 avg reward : 83.82073928002592\n",
      "after 480 avg reward : 69.7672552772989\n",
      "after 490 avg reward : 56.294604914715066\n",
      "after 500 avg reward : 133.52324723652566\n",
      "after 510 avg reward : 48.74601277335172\n",
      "after 520 avg reward : 120.783895239199\n",
      "after 530 avg reward : 159.26461998307076\n",
      "after 540 avg reward : 71.27840234532891\n",
      "after 550 avg reward : 25.9810706590384\n",
      "after 560 avg reward : 124.40654846778875\n",
      "after 570 avg reward : 170.04258767302682\n",
      "after 580 avg reward : 63.531873980763315\n",
      "after 590 avg reward : 183.50943346433033\n",
      "after 600 avg reward : 56.687434201328834\n",
      "after 610 avg reward : 108.40154922856041\n",
      "after 620 avg reward : 88.31851590383573\n",
      "after 630 avg reward : -10.655332975727884\n",
      "after 640 avg reward : 126.17672788001866\n",
      "after 650 avg reward : 99.5276415917691\n",
      "after 660 avg reward : 108.67702757214263\n",
      "after 670 avg reward : 76.24590276931335\n",
      "after 680 avg reward : 112.53668073020822\n",
      "after 690 avg reward : 46.03563219586222\n",
      "after 700 avg reward : 100.09738860267689\n",
      "after 710 avg reward : 51.087440535462704\n",
      "after 720 avg reward : -122.96152831139894\n",
      "after 730 avg reward : 107.19030558895051\n",
      "after 740 avg reward : 123.40209135669265\n",
      "after 750 avg reward : 108.74562447621743\n",
      "after 760 avg reward : 72.64584604163727\n",
      "after 770 avg reward : 77.297240508471\n",
      "after 780 avg reward : 150.78076663075768\n",
      "after 790 avg reward : 153.9796247161009\n",
      "after 800 avg reward : 66.95700350527397\n",
      "after 810 avg reward : 128.66148202499727\n",
      "after 820 avg reward : 102.74284680618663\n",
      "after 830 avg reward : 108.99319095520623\n",
      "after 840 avg reward : 108.36397200977244\n",
      "after 850 avg reward : 102.03238678460472\n",
      "after 860 avg reward : 98.62674043959711\n",
      "after 870 avg reward : 108.31228532695529\n",
      "after 880 avg reward : 98.18213922664495\n",
      "after 890 avg reward : 74.08630348268369\n",
      "after 900 avg reward : 62.914509596113625\n",
      "after 910 avg reward : 175.16353335019716\n",
      "after 920 avg reward : 164.46888134564233\n",
      "after 930 avg reward : 97.39908845170707\n",
      "after 940 avg reward : 117.23706161084297\n",
      "after 950 avg reward : 60.556881560364104\n",
      "after 960 avg reward : 109.727366767397\n",
      "after 970 avg reward : 189.47301233506053\n",
      "after 980 avg reward : 188.08220274780018\n",
      "after 990 avg reward : 79.40623573552685\n",
      "after 1000 avg reward : 104.10924238449984\n",
      "after 1010 avg reward : 70.13948851571652\n",
      "after 1020 avg reward : 113.47153079817579\n",
      "after 1030 avg reward : 85.31563906023358\n",
      "after 1040 avg reward : 75.26025839853455\n",
      "after 1050 avg reward : 162.55946325038295\n",
      "after 1060 avg reward : 77.68435691473587\n",
      "after 1070 avg reward : 61.41428350432204\n",
      "after 1080 avg reward : 90.91658548746304\n",
      "after 1090 avg reward : 182.78578585143782\n",
      "after 1100 avg reward : 95.63066414693228\n",
      "after 1110 avg reward : 184.49425165590964\n",
      "after 1120 avg reward : 158.51219768359857\n",
      "after 1130 avg reward : 150.52136975105626\n",
      "after 1140 avg reward : 96.14884533529123\n",
      "after 1150 avg reward : 136.2921046728822\n",
      "after 1160 avg reward : 93.5019298953408\n",
      "after 1170 avg reward : 65.96196682365687\n",
      "after 1180 avg reward : 137.6615784869478\n",
      "after 1190 avg reward : 177.29470060964516\n",
      "after 1200 avg reward : 59.24877905403148\n",
      "after 1210 avg reward : 187.15961804855954\n",
      "after 1220 avg reward : 40.068774230097205\n",
      "after 1230 avg reward : 130.20978807527996\n",
      "after 1240 avg reward : 99.09558144629787\n",
      "after 1250 avg reward : 109.81141035588419\n",
      "after 1260 avg reward : 116.01334256007688\n",
      "after 1270 avg reward : 105.27924821757135\n",
      "after 1280 avg reward : 160.79386268478805\n",
      "after 1290 avg reward : 83.02243363765245\n"
     ]
    },
    {
     
    }
   ],
   "source": [
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentSAC(act_dim, act_limit, state_dim)\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "    #for steps in range(num_steps):\n",
    "        while count < 300:\n",
    "            action,_ =  agent.policy.eval_n(state[np.newaxis])\n",
    "            #print(action)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            #print(f' s: {state} actins {action} reward {reward} next states : {next_state} done : {done}')\n",
    "            buffer.add(state, action[0], reward, next_state, done)\n",
    "\n",
    "            count += 1\n",
    "            if count % 5 == 0:\n",
    "              agent.train()\n",
    "              \n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            total_avg_reward.append(ret)               \n",
    "            if done:\n",
    "                break\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {steps} avg reward : {avg_rew}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, num_test_episodes, max_ep_len):\n",
    "    ep_rets, ep_lens = [], []\n",
    "    for j in range(num_test_episodes):\n",
    "        state, done, ep_ret, ep_len = env.reset(), False, 0, 0\n",
    "        while not(done or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            ##env.render()\n",
    "            #print(state)\n",
    "            act1,_ = agent.policy.eval_n(state[np.newaxis])\n",
    "            #print(act)\n",
    "            state, reward, done, _ = env.step(act1[0])\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "        ep_rets.append(ep_ret)\n",
    "        ep_lens.append(ep_len)\n",
    "    return np.mean(ep_rets), np.mean(ep_lens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fae32586ff03f47a58c920ef61c568276f9b28096f0a46988759f2e818341ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
