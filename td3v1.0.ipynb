{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <LunarLanderContinuous<LunarLanderContinuous-v2>>\n",
      "State shape: (8,)\n",
      "Action shape: (2,)\n",
      "action space Box([-1. -1.], [1. 1.], (2,), float32) observation space : Box([-inf -inf -inf -inf -inf -inf -inf -inf], [inf inf inf inf inf inf inf inf], (8,), float32)\n",
      "action space bound :[-1. -1.], [1. 1.]\n",
      "action limit = 1.0 dimension 2\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "env_name = 'LunarLanderContinuous-v2' # 'Pendulum-v0' #\n",
    "\n",
    "env = make_env(env_name)\n",
    "\n",
    "#plt.imshow(env.render(\"rgb_array\"))\n",
    "\n",
    "#env = env.reset()\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim, act_limit):\n",
    "        super(Policy, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb2 = tf.keras.layers.BatchNormalization()\n",
    "        self.actor = tf.keras.layers.Dense(act_dim)\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fcb2(x)\n",
    "        x = self.actor(x)\n",
    "        x = tf.keras.activations.tanh(x)  # to output in range(-1,1)\n",
    "        x = self.act_limit * x\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "        #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.call(state)\n",
    "        ##print(actions)\n",
    "        if not evaluate:\n",
    "            actions += tf.clip_by_value(tf.random.normal(shape=[act_dim], mean=0.0, stddev=0.1),-0.2,0.2)\n",
    "\n",
    "        actions = (tf.clip_by_value(actions, -self.act_limit , self.act_limit))\n",
    "        #print(f'actions in ac : {actions}')\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        ##print(f'Tensor states {state_ts} actions : {action_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "\n",
    "        return state_ts, action_ts, reward_ts, next_state_ts, done_flags\n",
    "    def initialize_replay_buffer(self,env, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTD3:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.learning_rate_critic = 0.0002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic(state_dim,act_dim)\n",
    "        self.critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.target_critic1 = Critic(state_dim,act_dim)\n",
    "        self.target_critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.policy = Policy(state_dim,act_dim,act_limit)\n",
    "        self.target_policy = Policy(state_dim,act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.policy(s)\n",
    "        _ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions = self.target_policy(next_states) #self.target_policy.act(next_states)\n",
    "            noise = tf.clip_by_value(tf.random.normal(shape = target_actions.shape, mean=0, stddev=0.5),-0.5,0.5)\n",
    "            target_actions += noise\n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "\n",
    "            #critic_loss1 = tf.keras.losses.MSE(target_qval, qval1)\n",
    "            #critic_loss2 = tf.keras.losses.MSE(target_qval, qval2)\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        #print(f'target actions : {target_actions} states : {states} target qv1 : {target_qval1} target qv2 : {target_qval2} qval 1 : {qval1} qval2: {qval2} target_qval : {target_qval} critic loss1 : {critic_loss1} critic loss : {critic_loss2} noise: {noise}')\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_p_loss(self,states):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.policy(states, training=True)\n",
    "            policy_loss = - self.critic1(states,actions)\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "        #self.critic1.trainable = False\n",
    "\n",
    "        #if step % 2 == 0 :\n",
    "        p_loss = self.compute_p_loss(states)\n",
    "        \n",
    "        #self.critic1.trainable = True\n",
    "        \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 0 avg reward : -157.10742097780934\n",
      "after 10 avg reward : -198.29394262967477\n",
      "after 20 avg reward : -217.06266599606792\n",
      "after 30 avg reward : -217.61860251266143\n",
      "after 40 avg reward : -86.85464004663775\n",
      "after 50 avg reward : -203.14362086483365\n",
      "after 60 avg reward : -114.13596037953008\n",
      "after 70 avg reward : -197.40550834798245\n",
      "after 80 avg reward : -75.51687406737469\n",
      "after 90 avg reward : -62.4205912224038\n",
      "after 100 avg reward : -8.580910476869192\n",
      "after 110 avg reward : 0.5350254215104335\n",
      "after 120 avg reward : 28.833600361615233\n",
      "after 130 avg reward : -30.832760608294798\n",
      "after 140 avg reward : -31.78826063562254\n",
      "after 150 avg reward : -75.68155596151136\n",
      "after 160 avg reward : -26.589837282645284\n",
      "after 170 avg reward : -65.98136833434185\n",
      "after 180 avg reward : -78.46478983553729\n",
      "after 190 avg reward : -110.86152784589095\n",
      "after 200 avg reward : -45.89338051134833\n",
      "after 210 avg reward : -15.191191947959414\n",
      "after 220 avg reward : -110.68709572673461\n",
      "after 230 avg reward : -93.36629254136899\n",
      "after 240 avg reward : -168.60532001696726\n",
      "after 250 avg reward : -199.25778377729756\n",
      "after 260 avg reward : -254.54929931034695\n",
      "after 270 avg reward : -13.81974033381179\n",
      "after 280 avg reward : 37.94947195778023\n",
      "after 290 avg reward : -31.79039935437682\n",
      "after 300 avg reward : -70.7881854252627\n",
      "after 310 avg reward : -41.33747088532355\n",
      "after 320 avg reward : 29.679803739981416\n",
      "after 330 avg reward : -9.219726815363376\n",
      "after 340 avg reward : -12.06755715230963\n",
      "after 350 avg reward : -49.48252691947561\n",
      "after 360 avg reward : -1.3337227556796527\n",
      "after 370 avg reward : -4.2729743816258\n",
      "after 380 avg reward : -13.959587578176846\n",
      "after 390 avg reward : -22.25460636457686\n",
      "after 400 avg reward : 5.837902695967294\n",
      "after 410 avg reward : -10.027938604820037\n",
      "after 420 avg reward : -30.255582960699673\n",
      "after 430 avg reward : 1.3366392231797335\n",
      "after 440 avg reward : -21.466419105521744\n",
      "after 450 avg reward : -37.569038955391754\n",
      "after 460 avg reward : 11.880526100354937\n",
      "after 470 avg reward : 6.560708348271393\n",
      "after 480 avg reward : -27.01833885579834\n",
      "after 490 avg reward : 26.37336917707487\n",
      "after 500 avg reward : -32.14558958207139\n",
      "after 510 avg reward : -44.47723003212756\n",
      "after 520 avg reward : -10.763001127550464\n",
      "after 530 avg reward : -4.56129278621893\n",
      "after 540 avg reward : 11.99160293719434\n",
      "after 550 avg reward : -26.297814886478374\n",
      "after 560 avg reward : -18.44923457948027\n",
      "after 570 avg reward : -26.504546270487843\n",
      "after 580 avg reward : -31.89602072002731\n",
      "after 590 avg reward : -30.23255928070443\n",
      "after 600 avg reward : 2.513458438671303\n",
      "after 610 avg reward : -27.7371125041286\n",
      "after 620 avg reward : 2.6123091424473377\n",
      "after 630 avg reward : -14.08220487510252\n",
      "after 640 avg reward : -16.466380723568683\n",
      "after 650 avg reward : -7.146533357782692\n",
      "after 660 avg reward : -4.740953163108973\n",
      "after 670 avg reward : -22.263641272816578\n",
      "after 680 avg reward : -13.53401815205718\n",
      "after 690 avg reward : -31.733800997085645\n",
      "after 700 avg reward : -6.611161701810012\n",
      "after 710 avg reward : 1.3726760276057761\n",
      "after 720 avg reward : 23.244975153564564\n",
      "after 730 avg reward : 15.819930708272633\n",
      "after 740 avg reward : -47.50269336403449\n",
      "after 750 avg reward : -33.278555571574074\n",
      "after 760 avg reward : -36.76856290360968\n",
      "after 770 avg reward : -19.513459535485662\n",
      "after 780 avg reward : -23.31232696082113\n",
      "after 790 avg reward : -48.51490427415075\n",
      "after 800 avg reward : -43.27287312671994\n",
      "after 810 avg reward : -31.00658499677137\n",
      "after 820 avg reward : -37.78077373612554\n",
      "after 830 avg reward : -61.677305814724164\n",
      "after 840 avg reward : -40.44827715456528\n",
      "after 850 avg reward : -49.82590018501249\n",
      "after 860 avg reward : -21.96229172817823\n",
      "after 870 avg reward : -44.493928261564\n",
      "after 880 avg reward : -20.99851478043245\n",
      "after 890 avg reward : -10.177747643347844\n",
      "after 900 avg reward : -65.41136965596908\n",
      "after 910 avg reward : -58.36260849418646\n",
      "after 920 avg reward : -60.38175228925527\n",
      "after 930 avg reward : -40.75364195903546\n",
      "after 940 avg reward : -78.12347928531746\n",
      "after 950 avg reward : -46.13119773655739\n",
      "after 960 avg reward : -39.67063600627875\n",
      "after 970 avg reward : 2.4485104862891878\n",
      "after 980 avg reward : -51.44156247803426\n",
      "after 990 avg reward : -30.014709980510354\n",
      "after 1000 avg reward : -8.892096379725615\n",
      "after 1010 avg reward : -22.113767051595804\n",
      "after 1020 avg reward : -64.75987675614937\n",
      "after 1030 avg reward : -90.7690596105318\n",
      "after 1040 avg reward : -31.057934830528314\n",
      "after 1050 avg reward : -16.525002006950714\n",
      "after 1060 avg reward : -56.94041415180574\n",
      "after 1070 avg reward : -3.281683496755312\n",
      "after 1080 avg reward : 0.28841140205334526\n",
      "after 1090 avg reward : -17.231824307980457\n",
      "after 1100 avg reward : -29.844797219867836\n",
      "after 1110 avg reward : -27.653514465844427\n",
      "after 1120 avg reward : -10.425010863192018\n",
      "after 1130 avg reward : -30.73850772305995\n",
      "after 1140 avg reward : -21.935250936930466\n",
      "after 1150 avg reward : -16.587505373799157\n",
      "after 1160 avg reward : -28.09283017104334\n",
      "after 1170 avg reward : -31.915440249611066\n",
      "after 1180 avg reward : -55.22357329805111\n",
      "after 1190 avg reward : -30.049478099111262\n",
      "after 1200 avg reward : -55.562597558057504\n",
      "after 1210 avg reward : -48.334338486621675\n",
      "after 1220 avg reward : -38.048584165880285\n",
      "after 1230 avg reward : -30.584302898013846\n",
      "after 1240 avg reward : -58.586673460050996\n",
      "after 1250 avg reward : -21.530642394439298\n",
      "after 1260 avg reward : -65.96155186209928\n",
      "after 1270 avg reward : -31.561667340468443\n",
      "after 1280 avg reward : -15.908363289977752\n",
      "after 1290 avg reward : -35.90985606132564\n",
      "after 1300 avg reward : -69.11204138753814\n",
      "after 1310 avg reward : -43.66263062906651\n",
      "after 1320 avg reward : -12.71313032439886\n",
      "after 1330 avg reward : -25.62945314725524\n",
      "after 1340 avg reward : -24.738987808513833\n",
      "after 1350 avg reward : -25.13850143934784\n",
      "after 1360 avg reward : -36.23109652762468\n",
      "after 1370 avg reward : -8.806459559248058\n",
      "after 1380 avg reward : 1.607260483948771\n",
      "after 1390 avg reward : -4.793547108074658\n",
      "after 1400 avg reward : -17.35467572222958\n",
      "after 1410 avg reward : -10.725548008501514\n",
      "after 1420 avg reward : -34.510499092637914\n",
      "after 1430 avg reward : -51.14599716957237\n",
      "after 1440 avg reward : -69.10018458683415\n",
      "after 1450 avg reward : -42.76075083212608\n",
      "after 1460 avg reward : -16.71024473934056\n",
      "after 1470 avg reward : -13.218954561715236\n",
      "after 1480 avg reward : -26.481342393507738\n",
      "after 1490 avg reward : -33.30294511048726\n",
      "after 1500 avg reward : -25.318316225611206\n",
      "after 1510 avg reward : -31.691990228311834\n",
      "after 1520 avg reward : -3.715764425596747\n",
      "after 1530 avg reward : -8.75667193392116\n",
      "after 1540 avg reward : -46.00983269048926\n",
      "after 1550 avg reward : -35.90944530730777\n",
      "after 1560 avg reward : -16.59653500856099\n",
      "after 1570 avg reward : -8.20396778941317\n",
      "after 1580 avg reward : -38.81280038387116\n",
      "after 1590 avg reward : -25.963563318045836\n",
      "after 1600 avg reward : -46.72160475692802\n",
      "after 1610 avg reward : -55.5004363434315\n",
      "after 1620 avg reward : -10.794246258717106\n",
      "after 1630 avg reward : -37.24217801264437\n",
      "after 1640 avg reward : -42.9869084664181\n",
      "after 1650 avg reward : -50.0678435850721\n",
      "after 1660 avg reward : -44.02675253570453\n",
      "after 1670 avg reward : -35.68164681482485\n",
      "after 1680 avg reward : -31.85206686870374\n",
      "after 1690 avg reward : -59.206246265818706\n",
      "after 1700 avg reward : -5.338996036634375\n",
      "after 1710 avg reward : -39.72393457764594\n",
      "after 1720 avg reward : -57.45481699346833\n",
      "after 1730 avg reward : -25.194863195540513\n",
      "after 1740 avg reward : -4.22250489981773\n",
      "after 1750 avg reward : -21.785313760067503\n",
      "after 1760 avg reward : -22.36057935873642\n",
      "after 1770 avg reward : -21.146689410495775\n",
      "after 1780 avg reward : -24.055303119715\n",
      "after 1790 avg reward : -13.357084262867133\n",
      "after 1800 avg reward : -17.773986716897095\n",
      "after 1810 avg reward : -31.89574980755924\n",
      "after 1820 avg reward : -52.372532702778344\n",
      "after 1830 avg reward : -34.76449863593112\n",
      "after 1840 avg reward : -34.163368396404266\n",
      "after 1850 avg reward : -46.0835376141393\n",
      "after 1860 avg reward : -40.191646797336446\n",
      "after 1870 avg reward : -8.885993523878836\n",
      "after 1880 avg reward : -7.719265378676228\n",
      "after 1890 avg reward : -17.972561223484355\n",
      "after 1900 avg reward : -32.85876764116125\n",
      "after 1910 avg reward : -45.32186384486876\n",
      "after 1920 avg reward : -50.88273552003635\n",
      "after 1930 avg reward : -36.43255117243736\n",
      "after 1940 avg reward : -9.053319456217807\n",
      "after 1950 avg reward : -42.48424842831512\n",
      "after 1960 avg reward : -7.412727078928668\n",
      "after 1970 avg reward : -34.590345021136294\n",
      "after 1980 avg reward : -43.67718900131719\n",
      "after 1990 avg reward : -12.726021261422815\n",
      "after 2000 avg reward : -19.018184559550576\n",
      "after 2010 avg reward : -35.809645534471585\n",
      "after 2020 avg reward : -54.620618933066545\n",
      "after 2030 avg reward : -8.453299483791778\n",
      "after 2040 avg reward : -20.175961052287228\n",
      "after 2050 avg reward : -28.983563513240405\n",
      "after 2060 avg reward : -27.116139798602735\n",
      "after 2070 avg reward : -8.860181750935267\n",
      "after 2080 avg reward : -33.76891652581837\n",
      "after 2090 avg reward : -40.02067695165229\n",
      "after 2100 avg reward : -20.826891899996745\n",
      "after 2110 avg reward : -30.02598577940096\n",
      "after 2120 avg reward : -20.918701560716702\n",
      "after 2130 avg reward : -57.867516952910776\n",
      "after 2140 avg reward : -39.24396303449953\n",
      "after 2150 avg reward : -58.03131058532357\n",
      "after 2160 avg reward : -49.321954935358136\n",
      "after 2170 avg reward : -6.507054178145698\n",
      "after 2180 avg reward : 3.942648658038549\n",
      "after 2190 avg reward : -26.346133073291373\n",
      "after 2200 avg reward : -32.919340803784465\n",
      "after 2210 avg reward : -9.71173727185178\n",
      "after 2220 avg reward : -26.17391000731813\n",
      "after 2230 avg reward : -19.987933107402192\n",
      "after 2240 avg reward : -25.937165970947063\n",
      "after 2250 avg reward : -47.341183311062125\n",
      "after 2260 avg reward : -18.180674795980757\n",
      "after 2270 avg reward : -60.65063837876378\n",
      "after 2280 avg reward : -52.87678526701709\n",
      "after 2290 avg reward : -43.254533579776066\n",
      "after 2300 avg reward : -78.10456058383367\n",
      "after 2310 avg reward : -27.98517687130153\n",
      "after 2320 avg reward : -5.5471057393459215\n",
      "after 2330 avg reward : -5.371769911181215\n",
      "after 2340 avg reward : -10.46028522549621\n",
      "after 2350 avg reward : -24.580621560463786\n",
      "after 2360 avg reward : -29.143961107663056\n",
      "after 2370 avg reward : -36.97188761595985\n",
      "after 2380 avg reward : -20.509844621957477\n",
      "after 2390 avg reward : -20.997605041748855\n",
      "after 2400 avg reward : -11.163330540336478\n",
      "after 2410 avg reward : 2.977846628555245\n",
      "after 2420 avg reward : -17.970876329816875\n",
      "after 2430 avg reward : -55.61381205862604\n",
      "after 2440 avg reward : -62.616603017462886\n",
      "after 2450 avg reward : -47.05176244390807\n",
      "after 2460 avg reward : -35.885244743076974\n",
      "after 2470 avg reward : 15.42078927609721\n",
      "after 2480 avg reward : -40.056689936124016\n",
      "after 2490 avg reward : -21.466802506591197\n",
      "after 2500 avg reward : -55.46969717116773\n",
      "after 2510 avg reward : -43.875267059041946\n",
      "after 2520 avg reward : -15.15187154788274\n",
      "after 2530 avg reward : -26.460200766913836\n",
      "after 2540 avg reward : 0.14517381620129424\n",
      "after 2550 avg reward : -12.781859867737774\n",
      "after 2560 avg reward : -19.396159614204326\n",
      "after 2570 avg reward : -24.16620983923185\n",
      "after 2580 avg reward : -10.071838652405892\n",
      "after 2590 avg reward : 7.571487689368527\n",
      "after 2600 avg reward : -0.9633902841489249\n",
      "after 2610 avg reward : -12.977608056475969\n",
      "after 2620 avg reward : 13.715657934451192\n",
      "after 2630 avg reward : -1.5568754196642358\n",
      "after 2640 avg reward : -8.359121468693877\n",
      "after 2650 avg reward : -8.8590642100186\n",
      "after 2660 avg reward : -12.427580539981715\n",
      "after 2670 avg reward : -4.937831328271797\n",
      "after 2680 avg reward : -6.689174673747322\n",
      "after 2690 avg reward : -13.433386341993844\n",
      "after 2700 avg reward : -2.787083337866817\n",
      "after 2710 avg reward : 9.913201952930526\n",
      "after 2720 avg reward : -16.612173212307162\n",
      "after 2730 avg reward : -13.50455618276375\n",
      "after 2740 avg reward : -17.084046178792082\n",
      "after 2750 avg reward : 1.567356645051243\n",
      "after 2760 avg reward : -11.742983697655731\n",
      "after 2770 avg reward : 4.145746526512107\n",
      "after 2780 avg reward : 0.5487532040711557\n",
      "after 2790 avg reward : -5.027121801983459\n",
      "after 2800 avg reward : -33.61987831803765\n",
      "after 2810 avg reward : -10.53496297882923\n",
      "after 2820 avg reward : 10.972591595457509\n",
      "after 2830 avg reward : 5.796725389210768\n",
      "after 2840 avg reward : 6.712306590259845\n",
      "after 2850 avg reward : -6.403274713151484\n",
      "after 2860 avg reward : 7.0176284222376335\n",
      "after 2870 avg reward : 16.562810696702673\n",
      "after 2880 avg reward : -35.106565057792466\n",
      "after 2890 avg reward : 6.894929910433331\n",
      "after 2900 avg reward : -20.281182506322693\n",
      "after 2910 avg reward : -44.74151831930126\n",
      "after 2920 avg reward : 1.7309863788452122\n",
      "after 2930 avg reward : 7.7611186149419185\n",
      "after 2940 avg reward : -6.170964352472118\n",
      "after 2950 avg reward : -0.15953737469782397\n",
      "after 2960 avg reward : 5.2541123787855515\n",
      "after 2970 avg reward : -5.303586537020593\n",
      "after 2980 avg reward : -3.3693378133345036\n",
      "after 2990 avg reward : 4.9887310178006095\n",
      "after 3000 avg reward : -10.916377968224426\n",
      "after 3010 avg reward : 3.116641105675427\n",
      "after 3020 avg reward : -1.309978130448957\n",
      "after 3030 avg reward : 18.99065041773173\n",
      "after 3040 avg reward : 20.24778455635638\n",
      "after 3050 avg reward : -2.440801170235544\n",
      "after 3060 avg reward : -1.4485027971093558\n",
      "after 3070 avg reward : 7.589310874106795\n",
      "after 3080 avg reward : -12.737452072745985\n",
      "after 3090 avg reward : -24.44738772172702\n",
      "after 3100 avg reward : 13.642893158954077\n",
      "after 3110 avg reward : 5.059293048960234\n",
      "after 3120 avg reward : 4.584627870074064\n",
      "after 3130 avg reward : 3.3691011841018534\n",
      "after 3140 avg reward : -3.9847319291087424\n",
      "after 3150 avg reward : 13.9683104599713\n",
      "after 3160 avg reward : 9.779038436474213\n",
      "after 3170 avg reward : 11.767187439142578\n",
      "after 3180 avg reward : 1.4764669974886866\n",
      "after 3190 avg reward : -11.740756416852047\n",
      "after 3200 avg reward : -5.773688470732102\n",
      "after 3210 avg reward : 20.438846430308132\n",
      "after 3220 avg reward : -0.8899073743664883\n",
      "after 3230 avg reward : -11.883238194560843\n",
      "after 3240 avg reward : 6.604713456709466\n",
      "after 3250 avg reward : 8.98376529586161\n",
      "after 3260 avg reward : 7.147305487258092\n",
      "after 3270 avg reward : 9.486475498453427\n",
      "after 3280 avg reward : -13.738956606764011\n",
      "after 3290 avg reward : 20.81723425444656\n",
      "after 3300 avg reward : -7.115943376518747\n",
      "after 3310 avg reward : -2.430689992237623\n",
      "after 3320 avg reward : 10.315297924295288\n",
      "after 3330 avg reward : -0.3648564859644573\n",
      "after 3340 avg reward : -1.146592030860819\n",
      "after 3350 avg reward : 8.086841632466296\n",
      "after 3360 avg reward : 12.939997250222177\n",
      "after 3370 avg reward : 0.28066751601012785\n",
      "after 3380 avg reward : 9.513636170103128\n",
      "after 3390 avg reward : 16.759803291557404\n",
      "after 3400 avg reward : -3.9971466722452824\n",
      "after 3410 avg reward : 19.526514053559623\n",
      "after 3420 avg reward : 7.602457039861967\n",
      "after 3430 avg reward : 8.826603755358523\n",
      "after 3440 avg reward : -4.961883917984086\n",
      "after 3450 avg reward : 26.736055998702142\n",
      "after 3460 avg reward : 5.565807711821135\n",
      "after 3470 avg reward : -5.312157726707863\n",
      "after 3480 avg reward : -0.9018823391766488\n",
      "after 3490 avg reward : 0.8142850312870522\n",
      "after 3500 avg reward : 18.596511057814922\n",
      "after 3510 avg reward : 15.698863568947491\n",
      "after 3520 avg reward : 12.703219588877294\n",
      "after 3530 avg reward : -15.270761299070362\n",
      "after 3540 avg reward : 1.5520022178828548\n",
      "after 3550 avg reward : 14.230551391862553\n",
      "after 3560 avg reward : 0.3838932124996841\n",
      "after 3570 avg reward : 9.174540428379384\n",
      "after 3580 avg reward : -1.9735225589328962\n",
      "after 3590 avg reward : 4.737128323809708\n",
      "after 3600 avg reward : 19.89261349045825\n",
      "after 3610 avg reward : 19.323998682238493\n",
      "after 3620 avg reward : 8.38951466540175\n",
      "after 3630 avg reward : -12.40444970342294\n",
      "after 3640 avg reward : -4.844132785099459\n",
      "after 3650 avg reward : 17.164990138409653\n",
      "after 3660 avg reward : 13.22475711967249\n",
      "after 3670 avg reward : 12.444459622388374\n",
      "after 3680 avg reward : 11.382736193858921\n",
      "after 3690 avg reward : 4.417756950475552\n",
      "after 3700 avg reward : 8.25044521728437\n",
      "after 3710 avg reward : 5.360214843221878\n",
      "after 3720 avg reward : 8.18809709802124\n",
      "after 3730 avg reward : 23.110931144499364\n",
      "after 3740 avg reward : -4.86241944108425\n",
      "after 3750 avg reward : 3.4080572827104185\n",
      "after 3760 avg reward : 7.114500927578379\n",
      "after 3770 avg reward : -8.207860571942666\n",
      "after 3780 avg reward : 27.98789505063191\n",
      "after 3790 avg reward : -9.251992670705587\n",
      "after 3800 avg reward : 28.420951168892127\n",
      "after 3810 avg reward : 44.135613991649336\n",
      "after 3820 avg reward : -1.6370486386127436\n",
      "after 3830 avg reward : 53.651039792337386\n",
      "after 3840 avg reward : 18.36926373841381\n",
      "after 3850 avg reward : 35.04195890582286\n",
      "after 3860 avg reward : 25.99049423735111\n",
      "after 3870 avg reward : 30.041569128508335\n",
      "after 3880 avg reward : -20.256069771789342\n",
      "after 3890 avg reward : 21.48729737038463\n",
      "after 3900 avg reward : 4.754786172152973\n",
      "after 3910 avg reward : 19.167694306920367\n",
      "after 3920 avg reward : 6.366650735781014\n",
      "after 3930 avg reward : 29.36569976312826\n",
      "after 3940 avg reward : 29.707391834170984\n",
      "after 3950 avg reward : 24.038379220193157\n",
      "after 3960 avg reward : 7.458034189581712\n",
      "after 3970 avg reward : 31.41085584214043\n",
      "after 3980 avg reward : -4.3894205848892325\n",
      "after 3990 avg reward : -17.533651229845095\n",
      "after 4000 avg reward : -4.341369190770638\n",
      "after 4010 avg reward : 18.807157792749532\n",
      "after 4020 avg reward : 38.26299363795363\n",
      "after 4030 avg reward : -2.6095171280090006\n",
      "after 4040 avg reward : -10.215126258561062\n",
      "after 4050 avg reward : 6.852645012059948\n",
      "after 4060 avg reward : 15.245212807188324\n",
      "after 4070 avg reward : 3.4276746578505026\n",
      "after 4080 avg reward : -10.006731306829817\n",
      "after 4090 avg reward : 37.69200489167993\n",
      "after 4100 avg reward : 17.7152549000377\n",
      "after 4110 avg reward : 8.154301317290532\n",
      "after 4120 avg reward : 11.787014616573199\n",
      "after 4130 avg reward : 27.329223111289224\n",
      "after 4140 avg reward : 37.56958946698207\n",
      "after 4150 avg reward : 0.7463378873155613\n",
      "after 4160 avg reward : 2.2528587277556658\n",
      "after 4170 avg reward : 26.83405734113839\n",
      "after 4180 avg reward : 10.273892296325894\n",
      "after 4190 avg reward : 48.8870112602679\n",
      "after 4200 avg reward : -6.683701250979463\n",
      "after 4210 avg reward : -58.00575733086189\n",
      "after 4220 avg reward : -12.734582136705336\n",
      "after 4230 avg reward : 44.01872766876597\n",
      "after 4240 avg reward : 26.839979959740845\n",
      "after 4250 avg reward : 5.884590721953737\n",
      "after 4260 avg reward : 20.956165721894553\n",
      "after 4270 avg reward : -8.568309449525792\n",
      "after 4280 avg reward : 7.295136449465616\n",
      "after 4290 avg reward : 2.969549181601484\n",
      "after 4300 avg reward : 26.21920528783121\n",
      "after 4310 avg reward : 25.434931294960194\n",
      "after 4320 avg reward : 7.375526521395654\n",
      "after 4330 avg reward : 23.771987406121717\n",
      "after 4340 avg reward : 41.83049873092039\n",
      "after 4350 avg reward : 18.931718173494062\n",
      "after 4360 avg reward : 0.8226961864340104\n",
      "after 4370 avg reward : -7.138241493813398\n",
      "after 4380 avg reward : 14.270532970230866\n",
      "after 4390 avg reward : 10.877192230584402\n",
      "after 4400 avg reward : 15.310270596591334\n",
      "after 4410 avg reward : 10.598216109363118\n",
      "after 4420 avg reward : 10.358339457235271\n",
      "after 4430 avg reward : 25.48105470475201\n",
      "after 4440 avg reward : 24.05223640011159\n",
      "after 4450 avg reward : 13.169420320866687\n",
      "after 4460 avg reward : 9.440231866695125\n",
      "after 4470 avg reward : -0.6673216108229412\n",
      "after 4480 avg reward : -23.791827898505122\n",
      "after 4490 avg reward : -18.325659699320163\n",
      "after 4500 avg reward : -16.4484495239699\n",
      "after 4510 avg reward : 20.737371026146366\n",
      "after 4520 avg reward : 10.433628049945096\n",
      "after 4530 avg reward : -6.637430479233274\n",
      "after 4540 avg reward : 25.576004106419305\n",
      "after 4550 avg reward : 16.76548002527938\n",
      "after 4560 avg reward : 24.83196679352303\n",
      "after 4570 avg reward : 13.8098756388276\n",
      "after 4580 avg reward : 23.447052865283776\n",
      "after 4590 avg reward : -9.228468102420326\n",
      "after 4600 avg reward : -17.214559768539804\n",
      "after 4610 avg reward : 1.4418451505035137\n",
      "after 4620 avg reward : -11.311034021727412\n",
      "after 4630 avg reward : 2.382134709281894\n",
      "after 4640 avg reward : 1.671665256613041\n",
      "after 4650 avg reward : 0.09169324565822015\n",
      "after 4660 avg reward : -2.348620768333501\n",
      "after 4670 avg reward : 13.584563222460016\n",
      "after 4680 avg reward : 8.223463067162537\n",
      "after 4690 avg reward : 17.066265966558337\n",
      "after 4700 avg reward : 17.6592574985851\n",
      "after 4710 avg reward : 32.58135587371254\n",
      "after 4720 avg reward : 14.634761946315658\n",
      "after 4730 avg reward : 28.57881921536989\n",
      "after 4740 avg reward : 22.139663041164475\n",
      "after 4750 avg reward : 15.549136532954577\n",
      "after 4760 avg reward : 5.3962802692422\n",
      "after 4770 avg reward : 27.474892645313748\n",
      "after 4780 avg reward : 36.70723877728646\n",
      "after 4790 avg reward : 35.86966205087367\n",
      "after 4800 avg reward : 20.368925533622097\n",
      "after 4810 avg reward : 1.0251898565517092\n",
      "after 4820 avg reward : 11.521991045430966\n",
      "after 4830 avg reward : 7.437272936232904\n",
      "after 4840 avg reward : 15.692682979453599\n",
      "after 4850 avg reward : 33.08118579261729\n",
      "after 4860 avg reward : 4.492207049192517\n",
      "after 4870 avg reward : 17.072946230531265\n",
      "after 4880 avg reward : 34.384388800608825\n",
      "after 4890 avg reward : 20.917609123758673\n",
      "after 4900 avg reward : 31.57226975294609\n",
      "after 4910 avg reward : 29.443087088361587\n",
      "after 4920 avg reward : 35.34663353192675\n",
      "after 4930 avg reward : 20.741108176052258\n",
      "after 4940 avg reward : 27.628136330290943\n",
      "after 4950 avg reward : 34.138087109529195\n",
      "after 4960 avg reward : 38.59709922563097\n",
      "after 4970 avg reward : 10.370525448354394\n",
      "after 4980 avg reward : 37.380123653714264\n",
      "after 4990 avg reward : 35.31689073742131\n",
      "after 5000 avg reward : 32.62812082714473\n",
      "after 5010 avg reward : 21.136316713672315\n",
      "after 5020 avg reward : 27.27385707457746\n",
      "after 5030 avg reward : 30.37884123976525\n",
      "after 5040 avg reward : 23.070507724874407\n",
      "after 5050 avg reward : 33.85005626460134\n",
      "after 5060 avg reward : 20.9901858109217\n",
      "after 5070 avg reward : 29.04158190133619\n",
      "after 5080 avg reward : 37.31417826312475\n",
      "after 5090 avg reward : 32.020516033952\n",
      "after 5100 avg reward : 17.88891751418205\n",
      "after 5110 avg reward : 25.72160864987516\n",
      "after 5120 avg reward : 0.462054189898474\n",
      "after 5130 avg reward : 42.96880604552905\n",
      "after 5140 avg reward : 17.61938328051481\n",
      "after 5150 avg reward : 36.882858768896256\n",
      "after 5160 avg reward : 25.18929063810437\n",
      "after 5170 avg reward : 40.2608193366645\n",
      "after 5180 avg reward : 18.704601320495225\n",
      "after 5190 avg reward : 41.01064774954829\n",
      "after 5200 avg reward : 27.0396706238079\n",
      "after 5210 avg reward : 22.166465136117054\n",
      "after 5220 avg reward : 31.702486486473692\n",
      "after 5230 avg reward : 41.92018185451096\n",
      "after 5240 avg reward : 47.63669929109989\n",
      "after 5250 avg reward : 41.281218835298894\n",
      "after 5260 avg reward : 32.09333924933644\n",
      "after 5270 avg reward : 24.04535408244046\n",
      "after 5280 avg reward : 27.676945089944116\n",
      "after 5290 avg reward : 35.17245264110308\n",
      "after 5300 avg reward : 16.474795756482575\n",
      "after 5310 avg reward : -2.6571826895161075\n",
      "after 5320 avg reward : 17.28617135864971\n",
      "after 5330 avg reward : 11.450316397778053\n",
      "after 5340 avg reward : 20.747103749850893\n",
      "after 5350 avg reward : 45.49512021320556\n",
      "after 5360 avg reward : 13.968343139674854\n",
      "after 5370 avg reward : 31.290552940017086\n",
      "after 5380 avg reward : 46.707714671677614\n",
      "after 5390 avg reward : 25.028204766244876\n",
      "after 5400 avg reward : 44.78534923005582\n",
      "after 5410 avg reward : 37.770199606314605\n",
      "after 5420 avg reward : 47.647158373441286\n",
      "after 5430 avg reward : 48.51846008863036\n",
      "after 5440 avg reward : 28.4221999511986\n",
      "after 5450 avg reward : 33.7800385488159\n",
      "after 5460 avg reward : 26.926308549553255\n",
      "after 5470 avg reward : 29.281830577343744\n",
      "after 5480 avg reward : 30.245302332377285\n",
      "after 5490 avg reward : 18.924530488082794\n",
      "after 5500 avg reward : 31.050505797402348\n",
      "after 5510 avg reward : 30.498988126811707\n",
      "after 5520 avg reward : 2.955619655456054\n",
      "after 5530 avg reward : 36.65933508018465\n",
      "after 5540 avg reward : 28.611772420195017\n",
      "after 5550 avg reward : 35.69412795962839\n",
      "after 5560 avg reward : 23.337983500080504\n",
      "after 5570 avg reward : 38.53400018395686\n",
      "after 5580 avg reward : 43.08203509445694\n",
      "after 5590 avg reward : 45.2518087142095\n",
      "after 5600 avg reward : 32.386404200203366\n",
      "after 5610 avg reward : 20.644511906178344\n",
      "after 5620 avg reward : 27.74695112420677\n",
      "after 5630 avg reward : 25.891650709143867\n",
      "after 5640 avg reward : 26.582911838693985\n",
      "after 5650 avg reward : 35.385967264098745\n",
      "after 5660 avg reward : 56.43405445992495\n",
      "after 5670 avg reward : 30.522648799293144\n",
      "after 5680 avg reward : 32.36443436310088\n",
      "after 5690 avg reward : 24.966757348871926\n",
      "after 5700 avg reward : 29.325653058522356\n",
      "after 5710 avg reward : 14.604769884829711\n",
      "after 5720 avg reward : 23.33808799663296\n",
      "after 5730 avg reward : 37.96410632884097\n",
      "after 5740 avg reward : 22.935448686587872\n",
      "after 5750 avg reward : 40.346216510602424\n",
      "after 5760 avg reward : 53.44919116376534\n",
      "after 5770 avg reward : 31.724277562384678\n",
      "after 5780 avg reward : 37.14322288914821\n",
      "after 5790 avg reward : 46.56627408878092\n",
      "after 5800 avg reward : 28.796897858350356\n",
      "after 5810 avg reward : 58.70247353234015\n",
      "after 5820 avg reward : 22.435884953329655\n",
      "after 5830 avg reward : 52.540290557078194\n",
      "after 5840 avg reward : 40.96035932086454\n",
      "after 5850 avg reward : 53.61273254859786\n",
      "after 5860 avg reward : 40.616364547489994\n",
      "after 5870 avg reward : 47.224043647332245\n",
      "after 5880 avg reward : 30.03876427317178\n",
      "after 5890 avg reward : 25.41581398014172\n",
      "after 5900 avg reward : 47.68369245257275\n",
      "after 5910 avg reward : 35.76679830256748\n",
      "after 5920 avg reward : 28.66522986628877\n",
      "after 5930 avg reward : 42.24657872435044\n",
      "after 5940 avg reward : 62.73379476419909\n",
      "after 5950 avg reward : 50.37226486689703\n",
      "after 5960 avg reward : 57.055229014359654\n",
      "after 5970 avg reward : 30.552333588600884\n",
      "after 5980 avg reward : 39.099792619477306\n",
      "after 5990 avg reward : 50.49858115867653\n",
      "after 6000 avg reward : 42.6792149415689\n",
      "after 6010 avg reward : 44.848244387025844\n",
      "after 6020 avg reward : 49.575719986880614\n",
      "after 6030 avg reward : 50.36925602971026\n",
      "after 6040 avg reward : 64.49050905605614\n",
      "after 6050 avg reward : 41.27621192339092\n",
      "after 6060 avg reward : 70.10630091716828\n",
      "after 6070 avg reward : 45.20065405660059\n",
      "after 6080 avg reward : 45.83922434503458\n",
      "after 6090 avg reward : 47.72623680184102\n",
      "after 6100 avg reward : 44.62654525307113\n",
      "after 6110 avg reward : 32.341263751226194\n",
      "after 6120 avg reward : 46.64926129286543\n",
      "after 6130 avg reward : 57.334976847881634\n",
      "after 6140 avg reward : 64.62303578552121\n",
      "after 6150 avg reward : 63.94737337130365\n",
      "after 6160 avg reward : 78.57245655253308\n",
      "after 6170 avg reward : 68.5429368277677\n",
      "after 6180 avg reward : 55.70514089754461\n",
      "after 6190 avg reward : 48.203699423435104\n",
      "after 6200 avg reward : 49.879424383679556\n",
      "after 6210 avg reward : 63.941494176899766\n",
      "after 6220 avg reward : 60.786730949209115\n",
      "after 6230 avg reward : 52.00362542897801\n",
      "after 6240 avg reward : 56.47907774035757\n",
      "after 6250 avg reward : 57.09763343562719\n",
      "after 6260 avg reward : 55.585508000398896\n",
      "after 6270 avg reward : 70.9914007879194\n",
      "after 6280 avg reward : 22.192325730775707\n",
      "after 6290 avg reward : 52.86844998768298\n",
      "after 6300 avg reward : 47.75290486475916\n",
      "after 6310 avg reward : 63.122708061247614\n",
      "after 6320 avg reward : 52.13822224529298\n",
      "after 6330 avg reward : 61.015649708790214\n",
      "after 6340 avg reward : 70.30444099952376\n",
      "after 6350 avg reward : 55.379182455668946\n",
      "after 6360 avg reward : 74.44093150918278\n",
      "after 6370 avg reward : 75.70145073731291\n",
      "after 6380 avg reward : 68.15417364282557\n",
      "after 6390 avg reward : 69.74841575609211\n",
      "after 6400 avg reward : 73.66019078488578\n",
      "after 6410 avg reward : 61.47758061457606\n",
      "after 6420 avg reward : 66.35429296645258\n",
      "after 6430 avg reward : 78.03478363724165\n",
      "after 6440 avg reward : 67.1002288992797\n",
      "after 6450 avg reward : 51.91568208331589\n",
      "after 6460 avg reward : 87.52008353705017\n",
      "after 6470 avg reward : 54.08567948782498\n",
      "after 6480 avg reward : 55.7144262986979\n",
      "after 6490 avg reward : 70.60287059787467\n",
      "after 6500 avg reward : 82.27856405299819\n",
      "after 6510 avg reward : 71.99761326434287\n",
      "after 6520 avg reward : 41.7199974111049\n",
      "after 6530 avg reward : 63.69901009934897\n",
      "after 6540 avg reward : 91.26831167560391\n",
      "after 6550 avg reward : 62.08018811418172\n",
      "after 6560 avg reward : 72.19330043582424\n",
      "after 6570 avg reward : 71.95383383925174\n",
      "after 6580 avg reward : 73.21114930781857\n",
      "after 6590 avg reward : 72.92750878382881\n",
      "after 6600 avg reward : 79.39222072756198\n",
      "after 6610 avg reward : 76.1470793113961\n",
      "after 6620 avg reward : 80.18618050354456\n",
      "after 6630 avg reward : 60.38228245995991\n",
      "after 6640 avg reward : 42.93833790858325\n",
      "after 6650 avg reward : 63.63095139932799\n",
      "after 6660 avg reward : 47.277704737565884\n",
      "after 6670 avg reward : 53.05566353031857\n",
      "after 6680 avg reward : 79.15038508494479\n",
      "after 6690 avg reward : 69.16622732774219\n",
      "after 6700 avg reward : 64.56553308758085\n",
      "after 6710 avg reward : 63.965109831193715\n",
      "after 6720 avg reward : 75.68377202348356\n",
      "after 6730 avg reward : 76.58047967846396\n",
      "after 6740 avg reward : 54.15939486436432\n",
      "after 6750 avg reward : 61.96054062772653\n",
      "after 6760 avg reward : 80.20945947391264\n",
      "after 6770 avg reward : 64.42005071486189\n",
      "after 6780 avg reward : 58.11319903107511\n",
      "after 6790 avg reward : 58.160212103087545\n",
      "after 6800 avg reward : 61.34690074688167\n",
      "after 6810 avg reward : 52.32151461856024\n",
      "after 6820 avg reward : 62.546998098848974\n",
      "after 6830 avg reward : 60.78462560178823\n",
      "after 6840 avg reward : 73.32357375168075\n",
      "after 6850 avg reward : 51.388598847980326\n",
      "after 6860 avg reward : 71.65921691921541\n",
      "after 6870 avg reward : 80.93065798762214\n",
      "after 6880 avg reward : 60.53914905886597\n",
      "after 6890 avg reward : 70.3878748847021\n",
      "after 6900 avg reward : 61.01003303649743\n",
      "after 6910 avg reward : 62.38285640159105\n",
      "after 6920 avg reward : 83.8582353122357\n",
      "after 6930 avg reward : 57.187460500216424\n",
      "after 6940 avg reward : 76.04785087088433\n",
      "after 6950 avg reward : 55.81430999797642\n",
      "after 6960 avg reward : 65.4832777405489\n",
      "after 6970 avg reward : 97.0587397341916\n",
      "after 6980 avg reward : 60.465334368400455\n",
      "after 6990 avg reward : 73.0793108204152\n",
      "after 7000 avg reward : 75.66496696824575\n",
      "after 7010 avg reward : 69.76318956513002\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-91273d8d235c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0msteps\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-bcf447d838eb>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, evaluate)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#state = tf.convert_to_tensor([state], dtype=tf.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mactions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;31m##print(actions)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-bcf447d838eb>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;31m#x = self.fcb1(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    820\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    821\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 822\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    823\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1143\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5604\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5605\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5606\u001b[1;33m         transpose_b)\n\u001b[0m\u001b[0;32m   5607\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5608\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# routine to generate 200 steps first then use td3 to learn\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_of_time_steps = 10000\n",
    "    num_eps = 10000\n",
    "    num_steps = 300\n",
    "    target = False\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env,act_dim,act_limit,state_dim)\n",
    "\n",
    "    \n",
    "    for eps in range(num_eps):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ret = 0\n",
    "        if target == True:\n",
    "            break\n",
    "        for steps in range(num_steps):\n",
    "            action = agent.policy.act(state[np.newaxis])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            buffer.add(state,action[0],reward,next_state,done)\n",
    "\n",
    "            if done: \n",
    "                ret += reward\n",
    "                break                \n",
    "            else:\n",
    "                state = next_state\n",
    "            ret += reward\n",
    "\n",
    "        total_avg_reward.append(ret)\n",
    "\n",
    "        for i in range(20):\n",
    "            agent.train_step(1) # 1 doesnt do anything\n",
    "            \n",
    "        if eps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {eps} avg reward : {avg_rew}')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 10 avg reward : 13.260727978754272\n",
      "after 20 avg reward : -163.77867393460915\n",
      "after 30 avg reward : -598.3378876256529\n",
      "after 40 avg reward : -148.21631531566314\n",
      "after 50 avg reward : -80.94401821634344\n",
      "after 60 avg reward : 61.796345210474534\n",
      "after 70 avg reward : 64.91512014371848\n",
      "after 80 avg reward : -17.13627204361641\n",
      "after 90 avg reward : -113.35710720090262\n",
      "after 100 avg reward : -35.625411339602685\n",
      "after 110 avg reward : 12.846908900119086\n",
      "after 120 avg reward : 36.424141126000805\n",
      "after 130 avg reward : -151.45428062482299\n",
      "after 140 avg reward : 88.73639208125277\n",
      "after 150 avg reward : -87.91887859990001\n",
      "after 160 avg reward : 102.22901257137947\n",
      "after 170 avg reward : -178.97500723712687\n",
      "after 180 avg reward : -61.74027312684304\n",
      "after 190 avg reward : -3.072984477290084\n",
      "after 200 avg reward : -33.60065932069819\n",
      "after 210 avg reward : -77.54038998398997\n",
      "after 220 avg reward : 31.169866205952655\n",
      "after 230 avg reward : 86.44513568680786\n",
      "after 240 avg reward : 30.069887591406143\n",
      "after 250 avg reward : 23.17967698739386\n",
      "after 260 avg reward : 5.588662836457755\n",
      "after 270 avg reward : 10.74020911846777\n",
      "after 280 avg reward : -34.10869662246352\n",
      "after 290 avg reward : 13.94916653632884\n",
      "after 300 avg reward : -56.06706033526103\n",
      "after 310 avg reward : 45.04088918499809\n",
      "after 320 avg reward : -95.55099481592075\n",
      "after 330 avg reward : -79.48399491305437\n",
      "after 340 avg reward : -57.45686424624021\n",
      "after 350 avg reward : -115.60815887876474\n",
      "after 360 avg reward : -153.8055217409381\n",
      "after 370 avg reward : -65.52505984018134\n",
      "after 380 avg reward : -75.58033023605701\n",
      "after 390 avg reward : -121.81333072629909\n",
      "after 400 avg reward : -28.85392778699528\n",
      "after 410 avg reward : -17.25894166933451\n",
      "after 420 avg reward : 130.8605389887021\n",
      "after 430 avg reward : 30.80418041713507\n",
      "after 440 avg reward : -29.232684482084558\n",
      "after 450 avg reward : -47.08940839176881\n",
      "after 460 avg reward : -44.85100356946318\n",
      "after 470 avg reward : 122.43018601332805\n",
      "after 480 avg reward : 77.7052615984893\n",
      "after 490 avg reward : -42.16790342421427\n",
      "after 500 avg reward : 2.0653234296913876\n",
      "after 510 avg reward : -104.40217697927885\n",
      "after 520 avg reward : -37.970304653082756\n",
      "after 530 avg reward : -45.61714820417831\n",
      "after 540 avg reward : -74.07562134292296\n",
      "after 550 avg reward : -56.35879051145815\n",
      "after 560 avg reward : -46.37984360157013\n",
      "after 570 avg reward : -90.93105805624722\n",
      "after 580 avg reward : 142.4353555019797\n",
      "after 590 avg reward : -106.89476048864815\n",
      "after 600 avg reward : 152.00780477824136\n",
      "after 610 avg reward : -16.742123709639763\n",
      "after 620 avg reward : -24.2040994608711\n",
      "after 630 avg reward : -51.02084711756693\n",
      "after 640 avg reward : -27.630463345955853\n",
      "after 650 avg reward : -117.08392981679508\n",
      "after 660 avg reward : -55.08354937125078\n",
      "after 670 avg reward : -19.128533243513377\n",
      "after 680 avg reward : -174.6845416195556\n",
      "after 690 avg reward : -19.248826329835982\n",
      "after 700 avg reward : -4.26844207809643\n",
      "after 710 avg reward : -36.47169841856699\n",
      "after 720 avg reward : 15.906485651650248\n",
      "after 730 avg reward : 4.532952115827496\n",
      "after 740 avg reward : 18.934893725986996\n",
      "after 750 avg reward : -32.07951604846427\n",
      "after 760 avg reward : -49.370129540890034\n",
      "after 770 avg reward : -24.429970507532126\n",
      "after 780 avg reward : -11.254244247933716\n",
      "after 790 avg reward : -74.58359117829374\n",
      "after 800 avg reward : 19.88709237998012\n",
      "after 810 avg reward : -18.97869986138967\n",
      "after 820 avg reward : 64.14409399370277\n",
      "after 830 avg reward : -59.15370678956358\n",
      "after 840 avg reward : -7.105200821100576\n",
      "after 850 avg reward : 1.3067206466853662\n",
      "after 860 avg reward : -71.0193393434856\n",
      "after 870 avg reward : -57.34230679471958\n",
      "after 880 avg reward : 21.419761389927817\n",
      "after 890 avg reward : -59.26914170641796\n",
      "after 900 avg reward : -54.972917800809\n",
      "after 910 avg reward : -10.87059280581443\n",
      "after 920 avg reward : -10.65367462755013\n",
      "after 930 avg reward : 9.716498544934646\n",
      "after 940 avg reward : -52.33736996156527\n",
      "after 950 avg reward : 4.364961782500943\n",
      "after 960 avg reward : 13.567520736632861\n",
      "after 970 avg reward : -34.4800806997287\n",
      "after 980 avg reward : 77.6002842794754\n",
      "after 990 avg reward : -3.8183929975209834\n",
      "after 1000 avg reward : 147.94765312120575\n",
      "after 1010 avg reward : -25.225377421143584\n",
      "after 1020 avg reward : -14.535244668964292\n",
      "after 1030 avg reward : -63.44827680649979\n",
      "after 1040 avg reward : -52.81551344365813\n",
      "after 1050 avg reward : -10.017019249755062\n",
      "after 1060 avg reward : -16.99680330013694\n",
      "after 1070 avg reward : -2.561830562412967\n",
      "after 1080 avg reward : 159.93141156252182\n",
      "after 1090 avg reward : 103.14971128671054\n",
      "after 1100 avg reward : 133.0037889952218\n",
      "after 1110 avg reward : -42.405218939150224\n",
      "after 1120 avg reward : -67.98110012196885\n",
      "after 1130 avg reward : -27.595761269275005\n",
      "after 1140 avg reward : -78.6700761041557\n",
      "after 1150 avg reward : -98.77704671078406\n",
      "after 1160 avg reward : -23.253209257295914\n",
      "after 1170 avg reward : -36.99809776152771\n",
      "after 1180 avg reward : -9.469458797379705\n",
      "after 1190 avg reward : -45.44850964229678\n",
      "after 1200 avg reward : -52.63416284658952\n",
      "after 1210 avg reward : 18.778152482317534\n",
      "after 1220 avg reward : -17.362991726703115\n",
      "after 1230 avg reward : 9.724036235554266\n",
      "after 1240 avg reward : -23.454260532722156\n",
      "after 1250 avg reward : -19.618329685276645\n",
      "after 1260 avg reward : -45.754257496042825\n",
      "after 1270 avg reward : 49.406086003421805\n",
      "after 1280 avg reward : 74.61193371675822\n",
      "after 1290 avg reward : 18.313565008475994\n",
      "after 1300 avg reward : 53.573689523643154\n",
      "after 1310 avg reward : 34.90363023091693\n",
      "after 1320 avg reward : 8.7928763729082\n",
      "after 1330 avg reward : 33.35588064510253\n",
      "after 1340 avg reward : 196.86179374198872\n",
      "after 1350 avg reward : 60.18590672098738\n",
      "after 1360 avg reward : 46.70357681353478\n",
      "after 1370 avg reward : 155.31157358841568\n",
      "after 1380 avg reward : 122.60499289225193\n",
      "after 1390 avg reward : 127.89128661994957\n",
      "after 1400 avg reward : 186.08212581184617\n",
      "after 1410 avg reward : 139.8505395882788\n",
      "after 1420 avg reward : 70.31353581679983\n",
      "after 1430 avg reward : 146.51062288577586\n",
      "after 1440 avg reward : 135.80936779455024\n",
      "after 1450 avg reward : 162.410110795321\n",
      "after 1460 avg reward : 149.44202108099495\n",
      "after 1470 avg reward : 5.596963289544515\n",
      "after 1480 avg reward : 166.12052411103966\n",
      "after 1490 avg reward : 176.97698420839296\n",
      "after 1500 avg reward : 127.6227826228494\n",
      "after 1510 avg reward : 177.44969881782532\n",
      "after 1520 avg reward : 167.05066706546617\n",
      "after 1530 avg reward : 168.4639718165764\n",
      "after 1540 avg reward : 159.62904768161246\n",
      "after 1550 avg reward : 12.841523569792898\n",
      "after 1560 avg reward : 150.09432074004738\n",
      "after 1570 avg reward : 145.066478107926\n",
      "after 1580 avg reward : 186.94651684380597\n",
      "after 1590 avg reward : -0.05633726984497707\n",
      "after 1600 avg reward : 189.1116470722825\n",
      "after 1610 avg reward : 123.78083727504193\n",
      "after 1620 avg reward : 145.33670137835892\n",
      "after 1630 avg reward : 17.93882197681977\n",
      "after 1640 avg reward : 129.58376443595108\n",
      "after 1650 avg reward : 157.73491300022212\n",
      "after 1660 avg reward : 136.40966782036418\n",
      "after 1670 avg reward : 149.21176758114888\n",
      "after 1680 avg reward : 169.98209796376744\n",
      "after 1690 avg reward : 61.334209317725296\n",
      "after 1700 avg reward : 150.96302685748523\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__float__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    858\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__float__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-6cee95479808>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m               \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-febf78ee0c4e>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mp_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;31m#print(f'states: {states} actions : {actions} rewards : {rewards}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-3d802cf1b2c2>\u001b[0m in \u001b[0;36mto_tensors\u001b[1;34m(self, state_dim, act_dim)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;31m#print(type(states))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-3d802cf1b2c2>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#primary program\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env, act_dim, act_limit, state_dim)\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "    #for steps in range(num_steps):\n",
    "        while count < 900:\n",
    "            action =  agent.policy.act(state[np.newaxis])\n",
    "            #print(action)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            #print(f' s: {state} actins {action} reward {reward} next states : {next_state} done : {done}')\n",
    "            buffer.add(state, action[0], reward, next_state, done)\n",
    "\n",
    "            count += 1\n",
    "            if count % 5 == 0:\n",
    "              agent.train_step(count)\n",
    "              \n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            total_avg_reward.append(ret)               \n",
    "            if done:\n",
    "                break\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {steps} avg reward : {avg_rew}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, num_test_episodes, max_ep_len):\n",
    "    ep_rets, ep_lens = [], []\n",
    "    for j in range(num_test_episodes):\n",
    "        state, done, ep_ret, ep_len = env.reset(), False, 0, 0\n",
    "        while not(done or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            env.render()\n",
    "            #print(state)\n",
    "            act1 = agent.policy(state[np.newaxis])\n",
    "            #print(act)\n",
    "            state, reward, done, _ = env.step(act1[0])\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "        ep_rets.append(ep_ret)\n",
    "        ep_lens.append(ep_len)\n",
    "    return np.mean(ep_rets), np.mean(ep_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_env = gym.make(env_name)\n",
    "max_ep_len = []\n",
    "avg_ret, avg_len = test_agent(test_env, 5, 500)\n",
    "print(avg_ret,avg_len)\n",
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.17988046 -0.08516716]], shape=(1, 2), dtype=float32) tf.Tensor([ 0.17988046 -0.08516716], shape=(2,), dtype=float32)\n",
      "[ 0.0081089   1.3931643   0.40648142 -0.3900641  -0.00978398 -0.10092185\n",
      "  0.          0.        ] 1.4188150956543268 False\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "agent = AgentTD3(env, act_dim, act_limit, state_dim)\n",
    "a = agent.policy.act(state[np.newaxis])\n",
    "print(a, a[0])\n",
    "s,r,d,_ = env.step(a[0])\n",
    "print(s,r,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer.initialize_replay_buffer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target actions : [[-0.41776276  0.5692563 ]\n",
      " [ 0.45198345 -0.44281378]\n",
      " [-0.00935271  0.37258637]\n",
      " [ 0.35080436  0.5416223 ]\n",
      " [-0.53866404  0.5673858 ]\n",
      " [ 0.44856945  0.3217283 ]\n",
      " [ 0.12962672 -0.10596888]\n",
      " [-0.20377828  0.3279644 ]\n",
      " [ 0.18716666 -0.19694278]\n",
      " [-0.14255038  0.54002106]\n",
      " [ 0.25875282 -0.08139684]\n",
      " [-0.3290801  -0.24751492]\n",
      " [-0.20534638 -0.07291014]\n",
      " [-0.09311174  0.5486086 ]\n",
      " [ 0.01189423  0.26018274]\n",
      " [-0.5462302  -0.44804054]\n",
      " [ 0.30937284 -0.25923902]\n",
      " [-0.28620896 -0.07655201]\n",
      " [ 0.4272186   0.23154998]\n",
      " [-0.34163693  0.0508069 ]\n",
      " [-0.4028847   0.5300425 ]\n",
      " [ 0.07390745 -0.47447905]\n",
      " [-0.5885834  -0.15041621]\n",
      " [ 0.09707588 -0.19268093]\n",
      " [-0.6298606  -0.13908932]\n",
      " [ 0.46874967  0.5415466 ]\n",
      " [-0.50166255  0.58313894]\n",
      " [ 0.2885308  -0.22869042]\n",
      " [-0.0324495  -0.46281713]\n",
      " [-0.5478935   0.04819248]\n",
      " [-0.50449634  0.5204043 ]\n",
      " [ 0.431073   -0.23306853]] states : [[-3.60020578e-01  1.06553769e+00 -5.19126296e-01 -5.21923006e-01\n",
      "   1.23232618e-01  7.95961320e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-2.08444878e-01  1.33158171e+00 -4.64489132e-01 -2.60133475e-01\n",
      "   8.39409903e-02 -3.87756750e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.75433540e-01  1.07626343e+00  8.09311390e-01 -5.63555539e-01\n",
      "  -6.80553317e-01 -1.70828059e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-8.13558638e-01  6.79337919e-01 -1.44249988e+00 -9.82806623e-01\n",
      "   8.92118156e-01  2.80896991e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-3.70202720e-01  1.32816482e+00 -7.21187532e-01 -5.64331830e-01\n",
      "   3.67098540e-01  1.94578812e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-1.16512395e-01 -7.12636951e-03 -5.18212557e-01 -6.34663939e-01\n",
      "   5.14842510e-01 -2.55343747e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-1.15279481e-01  1.24030626e+00 -6.08689964e-01 -4.98901159e-01\n",
      "   1.39905229e-01  2.31452256e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-5.28919213e-02  1.47489429e+00 -5.63428879e-01  2.07980260e-01\n",
      "   8.51866007e-02  2.45155454e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-2.09835574e-01  5.08773804e-01 -2.81652629e-01 -9.68755722e-01\n",
      "  -2.10813329e-01 -9.19501632e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 2.75433540e-01  1.07626343e+00  8.09311390e-01 -5.63555539e-01\n",
      "  -6.80553317e-01 -1.70828059e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 8.40509772e-01  1.32449079e+00  1.46968734e+00 -6.88237548e-01\n",
      "  -9.36135650e-01 -1.57734662e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-9.92765464e-03  1.44333124e+00 -3.39637309e-01  4.53621775e-01\n",
      "   1.26278261e-02  1.01529300e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-4.54743385e-01  8.74278545e-01 -1.04822409e+00 -7.03821957e-01\n",
      "   5.94206572e-01 -2.79083308e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-6.27658904e-01  3.18633348e-01 -7.67385900e-01 -9.03044641e-01\n",
      "   4.13454741e-01  9.80993882e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.14492420e-02  1.47670770e+00  9.17486101e-02  1.15159906e-01\n",
      "  -8.84369984e-02 -1.18693925e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-2.29401737e-01  1.53114676e+00 -5.08294940e-01 -2.53647298e-01\n",
      "   2.32655942e-01  6.95233196e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-4.17217672e-01  8.77321422e-01 -5.16346931e-01 -6.68709159e-01\n",
      "   1.54687896e-01  1.23171702e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-1.71708018e-01  1.12166476e+00 -6.75286949e-01 -6.52916133e-01\n",
      "   2.69607306e-01  3.24440211e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-3.84490907e-01  9.80073154e-01 -9.22767758e-01 -6.54078901e-01\n",
      "   5.85363984e-01  5.96083626e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-2.27638483e-01  8.21213007e-01 -6.13654137e-01 -8.35431397e-01\n",
      "   1.67608365e-01  5.77751696e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-3.84758934e-02  1.41676152e+00 -3.96021515e-01  1.29357236e-03\n",
      "   3.28464508e-02  1.10950366e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-9.74844918e-02  1.39513564e+00 -4.80270386e-01 -7.46127069e-02\n",
      "   1.79020748e-01  3.17520708e-01  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.92864031e-01  7.03504980e-01  3.67596924e-01 -7.91876256e-01\n",
      "   1.26529634e-02  4.66297716e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-8.12753662e-02  1.47391820e+00 -4.13745075e-01  2.90823122e-03\n",
      "   8.84994194e-02 -1.05885705e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 3.82132918e-01  1.66349030e+00  9.59809184e-01 -4.92279753e-02\n",
      "  -5.20330191e-01 -2.15022534e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-2.85293907e-01  1.28854656e+00 -6.62907302e-01 -2.99017698e-01\n",
      "  -1.62619613e-02 -2.03752592e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-1.61288932e-01  1.01418424e+00 -5.92370868e-01 -7.28976965e-01\n",
      "   1.49070188e-01  2.71433443e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 9.01823044e-02  1.18033731e+00  5.54735720e-01 -7.04998672e-01\n",
      "  -1.34052873e-01 -1.64414272e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-5.47281146e-01  5.24043262e-01 -6.98365092e-01 -7.39987671e-01\n",
      "   3.55283618e-01  8.25659186e-02  0.00000000e+00  0.00000000e+00]\n",
      " [-2.81848371e-01  1.23241782e+00 -4.60634768e-01 -3.61341864e-01\n",
      "   7.44653866e-02  1.93389673e-02  0.00000000e+00  0.00000000e+00]\n",
      " [ 4.41966057e-02  1.46325457e+00  1.85907751e-01 -2.12140217e-01\n",
      "  -2.77017474e-01 -1.80703342e-01  0.00000000e+00  0.00000000e+00]\n",
      " [-3.60020578e-01  1.06553769e+00 -5.19126296e-01 -5.21923006e-01\n",
      "   1.23232618e-01  7.95961320e-02  0.00000000e+00  0.00000000e+00]] target qv1 : [ 0.06499328  0.0486935   0.00795722  0.13176422  0.07748052  0.24182813\n",
      "  0.01209816  0.03025287 -0.00563603  0.01362185  0.00132119 -0.01911836\n",
      "  0.08586251  0.07089931  0.00880276 -0.0144609   0.04995432 -0.00428635\n",
      "  0.09929155  0.0181075   0.0199093   0.01457847 -0.04967327  0.01882746\n",
      " -0.00276781  0.07073726  0.03783369 -0.00569499  0.05731333  0.03523319\n",
      "  0.02535867  0.05897542] target qv2 : [-0.09868646 -0.07356162 -0.03065865 -0.11655778 -0.1380328  -0.13409603\n",
      " -0.08802918 -0.07818243 -0.0459046  -0.02398068 -0.05511593 -0.04706552\n",
      " -0.07272138 -0.0698981  -0.08565137 -0.14509773 -0.07293054 -0.10672414\n",
      " -0.08375371 -0.07783449 -0.10908337 -0.07559812 -0.0619448  -0.07522731\n",
      " -0.06462543 -0.09835982 -0.09633771 -0.04078002 -0.06242967 -0.1209804\n",
      " -0.07921875 -0.07211444] qval 1 : [ 6.1252296e-02  4.6282794e-02 -1.0989429e-03  6.3161746e-02\n",
      "  6.3929908e-02  1.4685571e-01  7.1648667e-03  3.7047774e-02\n",
      " -1.3265727e-02 -1.0989429e-03  1.1988721e-02  4.9347281e-02\n",
      "  6.3122265e-02  7.4647203e-02  4.5358203e-05  5.3756177e-02\n",
      "  3.0669378e-02 -3.5211284e-02  7.2694577e-02  5.6912616e-02\n",
      " -1.6581042e-02  5.6293912e-02 -1.0458678e-03 -1.7177336e-02\n",
      " -4.4298787e-03  7.2699323e-02  4.4717651e-02 -3.8446214e-02\n",
      " -1.6557323e-03  4.7683232e-02  2.0464137e-03  6.1252296e-02] qval2: [-0.09210026 -0.06880582 -0.07247999 -0.08367728 -0.10717255 -0.10457212\n",
      " -0.10670752 -0.02253915 -0.09280486 -0.07247999 -0.06602438 -0.03511053\n",
      " -0.06495211 -0.12323062 -0.07624141 -0.10438074 -0.11615688 -0.12042002\n",
      " -0.12267923 -0.11328178 -0.07626668 -0.07397842 -0.09940238 -0.10812989\n",
      " -0.06642831 -0.12406925 -0.10289934 -0.08790416 -0.13810186 -0.10193698\n",
      " -0.0772787  -0.09210026] target_qval : [-1.4095664e+00  9.8951161e-02 -1.3579949e+00 -3.5796614e+00\n",
      " -1.6960808e+00 -1.0000000e+02 -1.8468283e+00 -3.8453367e+00\n",
      " -1.0743027e+00 -1.3513837e+00 -4.2524519e+00 -2.0890853e+00\n",
      " -2.0263224e+00 -1.6183019e+00  1.0975174e+00 -1.9050430e+00\n",
      " -1.6284339e+00 -2.6800187e+00 -1.5030185e+00 -1.9179431e-01\n",
      " -1.3208306e+00 -2.6766059e+00  3.4517157e+00 -5.8380298e-02\n",
      " -1.3689021e+00 -1.2804514e+00 -2.4058288e-01 -6.8132484e-01\n",
      " -2.8714619e+00 -1.1520933e+00 -5.2298790e-01 -1.3832600e+00] critic loss1 : 317.37091064453125 critic loss : 315.48370361328125 noise: [[-0.37523106  0.5       ]\n",
      " [ 0.5        -0.5       ]\n",
      " [ 0.14336485  0.3325653 ]\n",
      " [ 0.3075973   0.5       ]\n",
      " [-0.5         0.5       ]\n",
      " [ 0.1736716   0.01538982]\n",
      " [ 0.17995091 -0.15277173]\n",
      " [-0.17272493  0.31447238]\n",
      " [ 0.2228828  -0.28036588]\n",
      " [ 0.01016717  0.5       ]\n",
      " [ 0.5        -0.09820627]\n",
      " [-0.31660393 -0.26102284]\n",
      " [-0.1937338  -0.15109235]\n",
      " [-0.10268874  0.5       ]\n",
      " [ 0.03294757  0.2103246 ]\n",
      " [-0.5        -0.5       ]\n",
      " [ 0.3507961  -0.32162347]\n",
      " [-0.24797758 -0.11797106]\n",
      " [ 0.4503637   0.15967658]\n",
      " [-0.31361386 -0.02608347]\n",
      " [-0.35895333  0.5       ]\n",
      " [ 0.1101148  -0.5       ]\n",
      " [-0.5        -0.16419125]\n",
      " [ 0.13823469 -0.2327442 ]\n",
      " [-0.5        -0.21864127]\n",
      " [ 0.5         0.4377879 ]\n",
      " [-0.45904386  0.5       ]\n",
      " [ 0.39241776 -0.2641876 ]\n",
      " [-0.01473513 -0.5       ]\n",
      " [-0.5        -0.01268501]\n",
      " [-0.44575545  0.4558842 ]\n",
      " [ 0.4736047  -0.3023248 ]]\n"
     ]
    }
   ],
   "source": [
    "agent = AgentTD3(env, act_dim, act_limit, state_dim)\n",
    "\n",
    "pl,cl1,cl2 = agent.train_step(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fae32586ff03f47a58c920ef61c568276f9b28096f0a46988759f2e818341ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
