{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <PendulumEnv<Pendulum-v0>>\n",
      "State shape: (3,)\n",
      "Action shape: (1,)\n",
      "action space Box([-2.], [2.], (1,), float32) observation space : Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "action space bound :[-2.], [2.]\n",
      "action limit = 2.0 dimension 1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "env_name =  'Pendulum-v0' # 'LunarLanderContinuous-v2' #\n",
    "\n",
    "env = make_env(env_name)\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim, act_limit):\n",
    "        super(Policy, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb2 = tf.keras.layers.BatchNormalization()\n",
    "        self.actor = tf.keras.layers.Dense(act_dim)\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fcb2(x)\n",
    "        x = self.actor(x)\n",
    "        x = tf.keras.activations.tanh(x)  # to output in range(-1,1)\n",
    "        x = self.act_limit * x\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "        #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.call(state)\n",
    "        ##print(actions)\n",
    "        if not evaluate:\n",
    "            actions += tf.clip_by_value(tf.random.normal(shape=[act_dim], mean=0.0, stddev=0.1),-0.2,0.2)\n",
    "\n",
    "        actions = (tf.clip_by_value(actions, -self.act_limit , self.act_limit))\n",
    "        #print(f'actions in ac : {actions}')\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        ##print(f'Tensor states {state_ts} actions : {action_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "\n",
    "        return state_ts, action_ts, reward_ts, next_state_ts, done_flags\n",
    "    def initialize_replay_buffer(self,env, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTD3:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.learning_rate_critic = 0.0002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic(state_dim,act_dim)\n",
    "        self.critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.target_critic1 = Critic(state_dim,act_dim)\n",
    "        self.target_critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.policy = Policy(state_dim,act_dim,act_limit)\n",
    "        self.target_policy = Policy(state_dim,act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.policy(s)\n",
    "        _ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions = self.target_policy(next_states) #self.target_policy.act(next_states)\n",
    "            noise = tf.clip_by_value(tf.random.normal(shape = target_actions.shape, mean=0, stddev=0.5),-0.5,0.5)\n",
    "            target_actions += noise\n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "\n",
    "            #critic_loss1 = tf.keras.losses.MSE(target_qval, qval1)\n",
    "            #critic_loss2 = tf.keras.losses.MSE(target_qval, qval2)\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        #print(f'target actions : {target_actions} states : {states} target qv1 : {target_qval1} target qv2 : {target_qval2} qval 1 : {qval1} qval2: {qval2} target_qval : {target_qval} critic loss1 : {critic_loss1} critic loss : {critic_loss2} noise: {noise}')\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_p_loss(self,states):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.policy(states, training=True)\n",
    "            policy_loss = - self.critic1(states,actions)\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "        #self.critic1.trainable = False\n",
    "\n",
    "        #if step % 2 == 0 :\n",
    "        p_loss = self.compute_p_loss(states)\n",
    "        \n",
    "        #self.critic1.trainable = True\n",
    "        \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2  \n"
   ]
  },
   "source": [
    "# routine to generate 200 steps first then use td3 to learn\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_of_time_steps = 10000\n",
    "    num_eps = 10000\n",
    "    num_steps = 300\n",
    "    target = False\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env,act_dim,act_limit,state_dim)\n",
    "\n",
    "    \n",
    "    for eps in range(num_eps):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ret = 0\n",
    "        if target == True:\n",
    "            break\n",
    "        for steps in range(num_steps):\n",
    "            action = agent.policy.act(state[np.newaxis])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            buffer.add(state,action[0],reward,next_state,done)\n",
    "\n",
    "            if done: \n",
    "                ret += reward\n",
    "                break                \n",
    "            else:\n",
    "                state = next_state\n",
    "            ret += reward\n",
    "\n",
    "        total_avg_reward.append(ret)\n",
    "\n",
    "        for i in range(20):\n",
    "            agent.train_step(1) # 1 doesnt do anything\n",
    "            \n",
    "        if eps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {eps} avg reward : {avg_rew}')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "after 10 avg reward : -2688.892069356738\n",
      "Episode 15 return : -134.12087742327662\n"
     ]
    }
   ],
   "source": [
    "#primary program\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env, act_dim, act_limit, state_dim)\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "    #for steps in range(num_steps):\n",
    "        while count < 900:\n",
    "            action =  agent.policy.act(state[np.newaxis])\n",
    "            #print(action)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            #print(f' s: {state} actins {action} reward {reward} next states : {next_state} done : {done}')\n",
    "            buffer.add(state, action[0], reward, next_state, done)\n",
    "\n",
    "            count += 1\n",
    "            if count % 5 == 0:\n",
    "              agent.train_step(count)\n",
    "              \n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            total_avg_reward.append(ret)               \n",
    "            if done:\n",
    "                break\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {steps} avg reward : {avg_rew}')\n",
    "        if ret > -150: #specific for pendulum, change for different environments e.g. for lunarlander-v0 required reward should be greater than 150\n",
    "            print(f'Episode {eps} return : {ret}')\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, num_test_episodes, max_ep_len):\n",
    "    ep_rets, ep_lens = [], []\n",
    "    for j in range(num_test_episodes):\n",
    "        state, done, ep_ret, ep_len = env.reset(), False, 0, 0\n",
    "        while not(done or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            env.render()\n",
    "            #print(state)\n",
    "            act1 = agent.policy(state[np.newaxis])\n",
    "            #print(act)\n",
    "            state, reward, done, _ = env.step(act1[0])\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "        ep_rets.append(ep_ret)\n",
    "        ep_lens.append(ep_len)\n",
    "    return np.mean(ep_rets), np.mean(ep_lens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fae32586ff03f47a58c920ef61c568276f9b28096f0a46988759f2e818341ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
