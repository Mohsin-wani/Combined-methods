{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import time\n",
    "import glob\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name, seed=None):\n",
    "    # remove time limit wrapper from environment\n",
    "    env = gym.make(env_name).unwrapped\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env : <PendulumEnv<Pendulum-v0>>\n",
      "State shape: (3,)\n",
      "Action shape: (1,)\n",
      "action space Box([-2.], [2.], (1,), float32) observation space : Box([-1. -1. -8.], [1. 1. 8.], (3,), float32)\n",
      "action space bound :[-2.], [2.]\n",
      "action limit = 2.0 dimension 1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "env_name =  'Pendulum-v0' # 'LunarLanderContinuous-v2' #\n",
    "\n",
    "env = make_env(env_name)\n",
    "\n",
    "print(f'env : {env}')\n",
    "state_shape, action_shape = env.observation_space.shape, env.action_space.shape\n",
    "print('State shape: {}'.format(state_shape))\n",
    "print('Action shape: {}'.format(action_shape))\n",
    "print(f'action space {env.action_space} observation space : {env.observation_space}')\n",
    "print(f'action space bound :{env.action_space.low}, {env.action_space.high}')\n",
    "act_limit = env.action_space.high[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "print(f'action limit = {act_limit} dimension {act_dim}')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "print(state_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim, act_limit):\n",
    "        super(Policy, self).__init__()\n",
    "        self.act_limit = act_limit\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb2 = tf.keras.layers.BatchNormalization()\n",
    "        self.actor = tf.keras.layers.Dense(act_dim)\n",
    "    \n",
    "    def call(self, s):\n",
    "        x = self.fc1(s)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        #x = self.fcb2(x)\n",
    "        x = self.actor(x)\n",
    "        x = tf.keras.activations.tanh(x)  # to output in range(-1,1)\n",
    "        x = self.act_limit * x\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, evaluate=False):\n",
    "        #state = tf.convert_to_tensor([state], dtype=tf.float32)\n",
    "        actions = self.call(state)\n",
    "        ##print(actions)\n",
    "        if not evaluate:\n",
    "            actions += tf.clip_by_value(tf.random.normal(shape=[act_dim], mean=0.0, stddev=0.1),-0.2,0.2)\n",
    "\n",
    "        actions = (tf.clip_by_value(actions, -self.act_limit , self.act_limit))\n",
    "        #print(f'actions in ac : {actions}')\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, state_dim, act_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        #self.fcb1 = tf.keras.layers.BatchNormalization()\n",
    "        self.fc2 = tf.keras.layers.Dense(512, activation=\"relu\")\n",
    "        self.Q = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, s, a):\n",
    "        x = tf.concat([s,a], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        #x = self.fcb1(x)\n",
    "        x = self.fc2(x)\n",
    "        q = self.Q(x)\n",
    "        return tf.squeeze(q, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size=1e6):\n",
    "        self.size = size #max number of items in buffer\n",
    "        self.buffer =[] #array to holde buffer\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        item = (state, action, reward, next_state, done)\n",
    "        if len(self.buffer) < self.size:\n",
    "            self.buffer.append(item)\n",
    "        else:\n",
    "            self.buffer[self.next_id] = item\n",
    "        self.next_id = (self.next_id + 1) % self.size\n",
    "        \n",
    "    def sample(self, batch_size=32):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size)\n",
    "        samples = [self.buffer[i] for i in idxs]\n",
    "        states, actions, rewards, next_states, done_flags = list(zip(*samples))\n",
    "        return np.array(states,np.float32), np.array(actions,np.float32), np.array(rewards,np.float32), np.array(next_states,np.float32), np.array(done_flags)\n",
    "    \n",
    "    def to_tensors(self, state_dim, act_dim):\n",
    "        states, actions, rewards, next_states, done_flags = self.sample(32)\n",
    "        #print(type(states))\n",
    "        states = np.array(states,np.float32)\n",
    "        states = np.reshape(states, (-1, state_dim))\n",
    "    \n",
    "        actions = np.reshape(actions, (-1,act_dim))\n",
    "        rewards = np.reshape(rewards,(-1,1))\n",
    "        rewards = rewards.squeeze()\n",
    "\n",
    "        next_states = np.array(next_states,np.float32)\n",
    "        next_states = np.reshape(next_states, (-1, state_dim))\n",
    "    \n",
    "        done_flags = np.reshape(done_flags, (-1,1))\n",
    "        done_flags = np.squeeze(done_flags)\n",
    "\n",
    "        ##print(f' states {states} actions : {actions} rewards : {rewards}:  next_states {next_states} dones flags : {done_flags}')\n",
    "\n",
    "        state_ts = tf.convert_to_tensor(states, dtype= tf.float32)\n",
    "        action_ts = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "        reward_ts = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        next_state_ts = tf.convert_to_tensor(next_states,dtype=tf.float32)\n",
    "    \n",
    "        ##print(f'Tensor states {state_ts} actions : {action_ts} rewards : {reward_ts}:  next_states {next_state_ts} dones flags : {done_flags}')\n",
    "\n",
    "        return state_ts, action_ts, reward_ts, next_state_ts, done_flags\n",
    "    def initialize_replay_buffer(self,env, n_steps = 1000):\n",
    "        state = env.reset()\n",
    "        for _ in range(n_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(f' s: {state} action {action} reward {reward} next state : {next_state} done : {done}')\n",
    "            buffer.add(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                state = env.reset()\n",
    "            state = next_state\n",
    "buffer = ReplayBuffer(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentTD3:\n",
    "    def __init__(self,env, act_dim, act_limit, state_dim, learning_rate = 1e-3, gamma = 0.99, polyak = 0.95):\n",
    "        self.learning_rate_critic = 0.0002\n",
    "        self.learning_rate_policy = 1e-3\n",
    "        self.polyak = polyak\n",
    "        self.gamma = gamma\n",
    "        self.act_dim = act_dim\n",
    "        self.state_dim = state_dim\n",
    "        self.act_limit = act_limit\n",
    "\n",
    "        self.critic1 = Critic(state_dim,act_dim)\n",
    "        self.critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.target_critic1 = Critic(state_dim,act_dim)\n",
    "        self.target_critic2 = Critic(state_dim,act_dim)\n",
    "\n",
    "        self.policy = Policy(state_dim,act_dim,act_limit)\n",
    "        self.target_policy = Policy(state_dim,act_dim,act_limit)\n",
    "\n",
    "        s = env.reset()\n",
    "        a = env.action_space.sample()\n",
    "        s = s[np.newaxis]\n",
    "\n",
    "        _ = self.critic1(s,a[np.newaxis])\n",
    "        _ = self.critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.target_critic1(s,a[np.newaxis])\n",
    "        _ = self.target_critic2(s,a[np.newaxis])\n",
    "\n",
    "        _ = self.policy(s)\n",
    "        _ = self.target_policy(s)\n",
    "\n",
    "        self.target_critic1.set_weights(self.critic1.get_weights())\n",
    "        self.target_critic2.set_weights(self.critic2.get_weights())\n",
    "        self.target_policy.set_weights(self.policy.get_weights())\n",
    "\n",
    "        self.target_critic1.trainable = False\n",
    "        self.target_critic2.trainable = False\n",
    "        self.target_policy.trainable = False\n",
    "\n",
    "        self.policy_optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_policy)\n",
    "        self.critic_optimizer1 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "        self.critic_optimizer2 = tf.keras.optimizers.Adam(learning_rate=self.learning_rate_critic)\n",
    "   \n",
    "    def polyak_update(self, target_network, network):\n",
    "        updated_model_weights = []\n",
    "        for weights, target_weights in zip(network.get_weights(), target_network.get_weights()):\n",
    "            new_weights = self.polyak * target_weights + ((1-self.polyak) * weights)\n",
    "            updated_model_weights.append(new_weights)\n",
    "        target_network.set_weights(updated_model_weights)\n",
    "        \n",
    "    @tf.function\n",
    "    def compute_q_loss(self,states,actions, rewards, next_states, dones, gamma=0.99):\n",
    "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "            target_actions = self.target_policy(next_states) #self.target_policy.act(next_states)\n",
    "            noise = tf.clip_by_value(tf.random.normal(shape = target_actions.shape, mean=0, stddev=0.5),-0.5,0.5)\n",
    "            target_actions += noise\n",
    "            target_actions = (tf.clip_by_value(target_actions, -self.act_limit, self.act_limit))\n",
    "            target_qval1 = self.target_critic1(next_states,target_actions)\n",
    "            target_qval2 = self.target_critic2(next_states,target_actions)\n",
    "\n",
    "            qval1 = self.critic1(states, actions, training=True)\n",
    "            qval2 = self.critic2(states, actions, training=True)\n",
    "\n",
    "            target_next_qval = tf.math.minimum(target_qval1, target_qval2)\n",
    "\n",
    "            target_qval = rewards + gamma * (1-dones) * target_next_qval\n",
    "\n",
    "            #critic_loss1 = tf.keras.losses.MSE(target_qval, qval1)\n",
    "            #critic_loss2 = tf.keras.losses.MSE(target_qval, qval2)\n",
    "            \n",
    "            critic_loss1 = tf.reduce_mean((target_qval - qval1)**2)\n",
    "            critic_loss2 = tf.reduce_mean((target_qval - qval2)**2)\n",
    "        grads1 = tape1.gradient(critic_loss1, self.critic1.trainable_variables)\n",
    "        grads2 = tape2.gradient(critic_loss2, self.critic2.trainable_variables)\n",
    "        \n",
    "        self.critic_optimizer1.apply_gradients(zip(grads1,self.critic1.trainable_variables))       \n",
    "        self.critic_optimizer2.apply_gradients(zip(grads2,self.critic2.trainable_variables))\n",
    "\n",
    "        #print(f'target actions : {target_actions} states : {states} target qv1 : {target_qval1} target qv2 : {target_qval2} qval 1 : {qval1} qval2: {qval2} target_qval : {target_qval} critic loss1 : {critic_loss1} critic loss : {critic_loss2} noise: {noise}')\n",
    "\n",
    "        return critic_loss1, critic_loss2\n",
    "    \n",
    "    @tf.function\n",
    "    def compute_p_loss(self,states):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.policy(states, training=True)\n",
    "            policy_loss = - self.critic1(states,actions)\n",
    "            p_loss =  tf.math.reduce_mean(policy_loss)\n",
    "\n",
    "        grads = tape.gradient(p_loss,self.policy.trainable_variables)\n",
    "        self.policy_optimizer.apply_gradients(zip(grads,self.policy.trainable_variables))\n",
    "\n",
    "        #print(f'states : {states} actions : {actions} policy_loss : {policy_loss} p_loss : {p_loss}') \n",
    "\n",
    "        return p_loss\n",
    "\n",
    "    def train_step(self,step):\n",
    "        p_loss = 0\n",
    "        states, actions, rewards, next_states, dones = buffer.to_tensors(self.state_dim,self.act_dim)\n",
    "        #print(f'states: {states} actions : {actions} rewards : {rewards}')\n",
    "        done_flags = np.array(dones,np.float32)\n",
    "        c_loss1, c_loss2 = self.compute_q_loss(states,actions, rewards, next_states, done_flags)\n",
    "        #self.critic1.trainable = False\n",
    "\n",
    "        #if step % 2 == 0 :\n",
    "        p_loss = self.compute_p_loss(states)\n",
    "        \n",
    "        #self.critic1.trainable = True\n",
    "        \n",
    "        self.polyak_update(self.target_critic1, self.critic1)\n",
    "        self.polyak_update(self.target_critic2, self.critic2)\n",
    "        self.polyak_update(self.target_policy, self.policy)\n",
    "\n",
    "        return p_loss, c_loss1, c_loss2  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "after 0 avg reward : -1853.5307825172674\n",
      "after 10 avg reward : -2329.448146441626\n",
      "after 20 avg reward : -2377.240093549518\n",
      "after 30 avg reward : -1904.7965762898398\n",
      "after 40 avg reward : -1769.0427109616357\n",
      "after 50 avg reward : -1638.1037600693758\n",
      "after 60 avg reward : -1959.383066742875\n",
      "after 70 avg reward : -1738.4460022174183\n",
      "after 80 avg reward : -1136.0812027157704\n",
      "after 90 avg reward : -1503.3433786218534\n",
      "after 100 avg reward : -1680.2163913069016\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-91273d8d235c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 1 doesnt do anything\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0meps\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-107c59a3dce2>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[0mp_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m         \u001b[1;31m#print(f'states: {states} actions : {actions} rewards : {rewards}')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-3d802cf1b2c2>\u001b[0m in \u001b[0;36mto_tensors\u001b[1;34m(self, state_dim, act_dim)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;31m#print(type(states))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-3d802cf1b2c2>\u001b[0m in \u001b[0;36msample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0midxs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone_flags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdone_flags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_slice_helper\u001b[1;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    899\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[0;32m   1062\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_axis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1064\u001b[1;33m       shrink_axis_mask=shrink_axis_mask)\n\u001b[0m\u001b[0;32m   1065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1066\u001b[0m   \u001b[0mparent_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\GYMTFGPU\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice\u001b[1;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[0;32m   9499\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"begin_mask\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9500\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"end_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ellipsis_mask\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9501\u001b[1;33m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[0;32m   9502\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9503\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# routine to generate 200 steps first then use td3 to learn\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_of_time_steps = 10000\n",
    "    num_eps = 10000\n",
    "    num_steps = 300\n",
    "    target = False\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env,act_dim,act_limit,state_dim)\n",
    "\n",
    "    \n",
    "    for eps in range(num_eps):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        ret = 0\n",
    "        if target == True:\n",
    "            break\n",
    "        for steps in range(num_steps):\n",
    "            action = agent.policy.act(state[np.newaxis])\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            buffer.add(state,action[0],reward,next_state,done)\n",
    "\n",
    "            if done: \n",
    "                ret += reward\n",
    "                break                \n",
    "            else:\n",
    "                state = next_state\n",
    "            ret += reward\n",
    "\n",
    "        total_avg_reward.append(ret)\n",
    "\n",
    "        for i in range(20):\n",
    "            agent.train_step(1) # 1 doesnt do anything\n",
    "            \n",
    "        if eps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {eps} avg reward : {avg_rew}')\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer critic_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer critic_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer policy_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer dense_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "after 10 avg reward : -2688.892069356738\n",
      "Episode 15 return : -134.12087742327662\n"
     ]
    }
   ],
   "source": [
    "#primary program\n",
    "gamma = 0.99\n",
    "with tf.device('GPU:0'):\n",
    "\n",
    "    test_env = gym.make(env_name)\n",
    "    max_ep_len = []\n",
    "    loss_qval, loss_pval = [], []\n",
    "    ep_reward = []\n",
    "    total_avg_reward = []\n",
    "\n",
    "    num_episodes = 5000\n",
    "    num_steps = 0\n",
    "    target = False\n",
    "    steps = 0\n",
    "    buffer.initialize_replay_buffer(env)\n",
    "    agent = AgentTD3(env, act_dim, act_limit, state_dim)\n",
    "\n",
    "    for eps in range(num_episodes):\n",
    "        if target == True:\n",
    "            break\n",
    "        state = env.reset()\n",
    "        ret = 0\n",
    "        ep_reward = []\n",
    "        done = False\n",
    "        count = 0\n",
    "\n",
    "    #for steps in range(num_steps):\n",
    "        while count < 900:\n",
    "            action =  agent.policy.act(state[np.newaxis])\n",
    "            #print(action)\n",
    "            next_state, reward, done, _ = env.step(action[0])\n",
    "            #print(f' s: {state} actins {action} reward {reward} next states : {next_state} done : {done}')\n",
    "            buffer.add(state, action[0], reward, next_state, done)\n",
    "\n",
    "            count += 1\n",
    "            if count % 5 == 0:\n",
    "              agent.train_step(count)\n",
    "              \n",
    "            state = next_state\n",
    "            ret += reward\n",
    "            total_avg_reward.append(ret)               \n",
    "            if done:\n",
    "                break\n",
    "        steps += 1\n",
    "        if steps % 10 == 0:\n",
    "            avg_rew = np.mean(total_avg_reward[-10:])\n",
    "            print(f'after {steps} avg reward : {avg_rew}')\n",
    "        if ret > -150: #specific for pendulum, change for different environments e.g. for lunarlander-v0 required reward should be greater than 150\n",
    "            print(f'Episode {eps} return : {ret}')\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(env, num_test_episodes, max_ep_len):\n",
    "    ep_rets, ep_lens = [], []\n",
    "    for j in range(num_test_episodes):\n",
    "        state, done, ep_ret, ep_len = env.reset(), False, 0, 0\n",
    "        while not(done or (ep_len == max_ep_len)):\n",
    "            # Take deterministic actions at test time (noise_scale=0)\n",
    "            env.render()\n",
    "            #print(state)\n",
    "            act1 = agent.policy(state[np.newaxis])\n",
    "            #print(act)\n",
    "            state, reward, done, _ = env.step(act1[0])\n",
    "            ep_ret += reward\n",
    "            ep_len += 1\n",
    "        ep_rets.append(ep_ret)\n",
    "        ep_lens.append(ep_len)\n",
    "    return np.mean(ep_rets), np.mean(ep_lens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GYMTFGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1fae32586ff03f47a58c920ef61c568276f9b28096f0a46988759f2e818341ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
